{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import statements\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from math import *\n",
    "import itertools\n",
    "# from multiprocessing import Queue # might need this?\n",
    "# import Queue   # this is Queue in 2.7, but queue in 3...not actually called in script?\n",
    "# import copy\n",
    "from math import factorial\n",
    "import time\n",
    "import math # added this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy # added this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# these don't work -- why not? -- they more or less work if I put the FL script in the upper CorpusTools folder\n",
    "from corpustools.exceptions import FuncLoadError # this one worked after I made a Python 3 Jupyter notebook (http://ipython.readthedocs.io/en/stable/install/kernel_install.html)\n",
    "from corpustools.funcload.io import save_minimal_pairs # this one worked after I changed the file path to corpustools.funcload.io instead of just .io\n",
    "from corpustools.corpus.classes.lexicon import EnvironmentFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_minpair(first, second, corpus_context, segment_pairs, environment_filter): # Q: how do I find or figure out what type of object each of these parameters should be? e.g. is segment_pairs a list? a tuple? a list of tuples? (corpus_context and environment_filter presumably are defined elsewhere...but still, it would be useful to know what their structure is)\n",
    "    # according to the doc strings of minpair_fl (below): segment_pairs : list of length-2 tuples of str\n",
    "    \"\"\"Return True iff first/second are a minimal pair.\n",
    "    Checks that all segments in those words are identical OR a valid segment pair\n",
    "    (from segment_pairs) and fit the environment_filter, and that there is at least\n",
    "    one difference between first and second.\n",
    "    \"\"\"\n",
    "    first = getattr(first, corpus_context.sequence_type) # Q: how do I find out what .sequence_type does?\n",
    "        # in contextmanagers.py: sequence_type is a string that is defined as \"Sequence type to evaluate algorithms on (i.e., 'transcription')\"\n",
    "        # so this is saying get the attribute labeled in corpus_context.sequence_type (\"transcription\" or \"vowels\" or whatever) of the item named 'first'\n",
    "    second = getattr(second, corpus_context.sequence_type)\n",
    "    if len(first) != len(second):\n",
    "        return False\n",
    "    has_difference = False\n",
    "    for i in range(len(first)):\n",
    "        if first[i] == second[i]:\n",
    "            continue\n",
    "        elif (conflateable(first[i], second[i], segment_pairs) # this is defined below\n",
    "            and fits_environment(first, second, i, environment_filter)): # this is defined below\n",
    "            has_difference = True\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    if has_difference:\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conflateable(seg1, seg2, segment_pairs):\n",
    "    \"\"\"Return True iff seg1 and seg2 are exactly one of the segment pairs\n",
    "    in segment_pairs (ignoring ordering of either).\n",
    "\n",
    "    seg1 and seg2 will never be identical in the input.\n",
    "    \"\"\"\n",
    "    for segment_pair in segment_pairs:\n",
    "        seg_set = set(segment_pair)\n",
    "        if seg1 in seg_set and seg2 in seg_set:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fits_environment(w1, w2, index, environment_filter):\n",
    "    \"\"\"Return True iff for both w1 and w2 (tiers), the environment\n",
    "    of its i'th element fits passes the environment_filter.\n",
    "    \"\"\"\n",
    "    if not environment_filter:\n",
    "        return True\n",
    "\n",
    "    def ready_for_re(word, index):\n",
    "        w = [str(seg) for seg in word]\n",
    "        w[index] = '_'\n",
    "        return ' '.join(w)\n",
    "\n",
    "    w1 = ready_for_re(w1, index)\n",
    "    w2 = ready_for_re(w2, index)\n",
    "    env_re = make_environment_re(environment_filter) # this is defined below\n",
    "\n",
    "    return (bool(re.search(env_re, w1)) and bool(re.search(env_re, w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ready_for_re(word, index):\n",
    "        w = [str(seg) for seg in word]\n",
    "        w[index] = '_'\n",
    "        return ' '.join(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_environment_re(environment_filter): # again, need to know structure of env_filter\n",
    "    if environment_filter.lhs:\n",
    "        re_lhs = ' '.join(['('+('|'.join([seg for seg in position])+')') for position in environment_filter.lhs])\n",
    "        re_lhs = re_lhs.replace('#', '^')\n",
    "    else:\n",
    "        re_lhs = ''\n",
    "\n",
    "    if environment_filter.rhs:\n",
    "        re_rhs = ' '.join(['('+('|'.join([seg for seg in position])+')') for position in environment_filter.rhs])\n",
    "        re_rhs = re_rhs.replace('#', '$')\n",
    "    else:\n",
    "        re_rhs = ''\n",
    "\n",
    "    if re_lhs and not re_lhs.endswith('^)'):\n",
    "        re_lhs += ' '\n",
    "    if re_rhs and not re_rhs.endswith('($'):\n",
    "        re_rhs = ' ' + re_rhs\n",
    "    return re_lhs + '_' + re_rhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minpair_fl(corpus_context, segment_pairs,\n",
    "        relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = True, distinguish_homophones = False,\n",
    "        environment_filter = None,\n",
    "        stop_check = None, call_back = None):\n",
    "    \"\"\"Calculate the functional load of the contrast between two segments\n",
    "    as a count of minimal pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus_context : CorpusContext\n",
    "        Context manager for a corpus\n",
    "    segment_pairs : list of length-2 tuples of str\n",
    "        The pairs of segments to be conflated.\n",
    "    relative_count_to_relevant_sounds : bool, optional\n",
    "        If True, divide the number of minimal pairs by \n",
    "        by the total number of words that contain either of the two segments.\n",
    "        # changed the name of this from \"relative_count\" to \"relative_count_to_relevant_sounds\"\n",
    "        # set its default to False above\n",
    "    relative_count_to_whole_corpus : bool, optional\n",
    "        If True, divide the number of minimal pairs by the total number of words \n",
    "        in the corpus (regardless of whether those words contain the target sounds).\n",
    "        Defaults to True.\n",
    "    distinguish_homophones : bool, optional\n",
    "        If False, then you'll count sock~shock (sock=clothing) and\n",
    "        sock~shock (sock=punch) as just one minimal pair; but if True,\n",
    "        you'll overcount alternative spellings of the same word, e.g.\n",
    "        axel~actual and axle~actual. False is the value used by Wedel et al.\n",
    "    environment_filter : EnvironmentFilter\n",
    "        Allows the user to restrict the neutralization process to segments in\n",
    "        particular segmental contexts\n",
    "    stop_check : callable, optional\n",
    "        Optional function to check whether to gracefully terminate early\n",
    "    call_back : callable, optional\n",
    "        Optional function to supply progress information during the function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple(int or float, list)\n",
    "        Tuple of: 0. if `relative_count_to_relevant_sounds`==False and \n",
    "        `relative_count_to_whole_corpus`==False, an int of the raw number of\n",
    "        minimal pairs; if `relative_count_to_relevant_sounds`==True, a float of that\n",
    "        count divided by the total number of words in the corpus that\n",
    "        include either `s1` or `s2`; if `relative_count_to_whole_corpus`==True, a\n",
    "        float of the raw count divided by the total number of words in the corpus; \n",
    "        and 1. list of minimal pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    if stop_check is not None and stop_check():\n",
    "        return\n",
    "\n",
    "    ## Count the number of words in the corpus (needed if relative_count_to_whole_corpus is True)\n",
    "    num_words_in_corpus = len(corpus_context.corpus) \n",
    "    \n",
    "    ## Filter out words that have none of the target segments\n",
    "    ## (for relative_count_to_relevant_sounds as well as improving runtime)\n",
    "    contain_target_segment = []\n",
    "    if call_back is not None:\n",
    "        call_back('Finding words with the specified segments...')\n",
    "        call_back(0, len(corpus_context))\n",
    "        cur = 0\n",
    "\n",
    "    all_target_segments = list(itertools.chain.from_iterable(segment_pairs)) # creates a list of target segments from the list of tuples\n",
    "    for w in corpus_context: # loops through the words in the context manager?\n",
    "        if stop_check is not None and stop_check():\n",
    "            return\n",
    "        if call_back is not None:\n",
    "            cur += 1\n",
    "            if cur % 100 == 0:\n",
    "                call_back(cur)\n",
    "        tier = getattr(w, corpus_context.sequence_type)\n",
    "        if any([s in tier for s in all_target_segments]):\n",
    "                contain_target_segment.append(w)\n",
    "    if stop_check is not None and stop_check():\n",
    "        return\n",
    "\n",
    "    ## Find minimal pairs\n",
    "    minpairs = []\n",
    "    if call_back is not None:\n",
    "        call_back('Finding minimal pairs...')\n",
    "        if len(contain_target_segment) >= 2:\n",
    "            call_back(0,factorial(len(contain_target_segment))/(factorial(len(contain_target_segment)-2)*2))\n",
    "        cur = 0\n",
    "    for first, second in itertools.combinations(contain_target_segment, 2):\n",
    "        if stop_check is not None and stop_check():\n",
    "            return\n",
    "        if call_back is not None:\n",
    "            cur += 1\n",
    "            if cur % 100 == 0:\n",
    "                call_back(cur)\n",
    "        if is_minpair(first, second, corpus_context, segment_pairs, environment_filter):\n",
    "            ordered_pair = sorted([(first, getattr(first, corpus_context.sequence_type)),\n",
    "                                   (second, getattr(second, corpus_context.sequence_type))],\n",
    "                                   key = lambda x: x[1]) # sort by tier/transcription\n",
    "            minpairs.append(tuple(ordered_pair))\n",
    "\n",
    "    ## Generate output \n",
    "    if not distinguish_homophones:\n",
    "        actual_minpairs = {}\n",
    "\n",
    "        for pair in minpairs:\n",
    "            if stop_check is not None and stop_check():\n",
    "                return\n",
    "            key = (pair[0][1], pair[1][1]) # Keys are tuples of transcriptions\n",
    "            if key not in actual_minpairs:\n",
    "                actual_minpairs[key] = (pair[0][0], pair[1][0]) # Values are words\n",
    "            else:\n",
    "                pair_freq = pair[0][0].frequency + pair[1][0].frequency\n",
    "                existing_freq = actual_minpairs[key][0].frequency + \\\n",
    "                                actual_minpairs[key][1].frequency\n",
    "                if pair_freq > existing_freq:\n",
    "                    actual_minpairs[key] = (pair[0][0], pair[1][0])\n",
    "        result = sum((x[0].frequency + x[1].frequency)/2\n",
    "                    for x in actual_minpairs.values())\n",
    "    else:\n",
    "        result = sum((x[0][0].frequency + x[1][0].frequency)/2 for x in minpairs)\n",
    "\n",
    "    if relative_count_to_relevant_sounds and len(contain_target_segment) > 0:\n",
    "        result /= sum(x.frequency for x in contain_target_segment)\n",
    "        \n",
    "        # Q: do I need to put anything here between these if statements (like make it an else if)? or can I just have them back to back?\n",
    "        # relative_count_to_relevant_sounds and relative_count_to_whole_corpus won't both be true\n",
    "        # but both COULD be false, in which case, result should just stay as the raw count \n",
    "        # that I assume is returned above -- elif will ensure that only one runs; if will allow both to run\n",
    "        \n",
    "    elif relative_count_to_whole_corpus:\n",
    "        result = result / num_words_in_corpus\n",
    "    \n",
    "    return (result, minpairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function is weirdly named. It should really be something like\n",
    "# average_minpair_fl\n",
    "# It has also been changed so as to have two \"relativizer\" options:\n",
    "# one to words containing the relevant segments and one to all\n",
    "# words in the corpus.\n",
    "def relative_minpair_fl(corpus_context, segment,\n",
    "            relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = True, \n",
    "            distinguish_homophones = False, output_filename = None, environment_filter = None,\n",
    "            stop_check = None, call_back = None):\n",
    "    \"\"\"Calculate the average functional load of the contrasts between a\n",
    "    segment and all other segments, as a count of minimal pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus_context : CorpusContext\n",
    "        Context manager for a corpus\n",
    "    segment : str\n",
    "        The target segment.\n",
    "    relative_count_to_relevant_sounds : bool, optional\n",
    "        If True, divide the number of minimal pairs\n",
    "        by the total number of words that contain either of the two segments.\n",
    "    relative_count_to_whole_corpus : bool, optional\n",
    "        If True, divide the number of minimal pairs by the total number of words \n",
    "        in the corpus (regardless of whether those words contain the target sounds).\n",
    "        Defaults to True.\n",
    "    distinguish_homophones : bool, optional\n",
    "        If False, then you'll count sock~shock (sock=clothing) and\n",
    "        sock~shock (sock=punch) as just one minimal pair; but if True,\n",
    "        you'll overcount alternative spellings of the same word, e.g.\n",
    "        axel~actual and axle~actual. False is the value used by Wedel et al.\n",
    "    environment_filter : EnvironmentFilter\n",
    "        Allows the user to restrict the neutralization process to segments in\n",
    "        particular segmental contexts\n",
    "    stop_check : callable, optional\n",
    "        Optional function to check whether to gracefully terminate early\n",
    "    call_back : callable, optional\n",
    "        Optional function to supply progress information during the function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int or float\n",
    "        If `relative_count_to_relevant_sounds`==False and `relative_count_to_whole_corpus`==False,\n",
    "        returns an int of the raw number of\n",
    "        minimal pairs. If `relative_count_to_relevant_sounds`==True, returns a float of\n",
    "        that count divided by the total number of words in the corpus\n",
    "        that include either `s1` or `s2`. If `relative_count_to_whole_corpus`==True, a\n",
    "        float of the raw count divided by the total number of words in the corpus. \n",
    "    \"\"\"\n",
    "    all_segments = corpus_context.inventory\n",
    "    segment_pairs = [(segment,other.symbol) for other in all_segments\n",
    "                        if other.symbol != segment and other.symbol != '#']\n",
    "\n",
    "    results = []\n",
    "    to_output = []\n",
    "    for sp in segment_pairs:\n",
    "        res = minpair_fl(corpus_context, [sp],\n",
    "            relative_count_to_relevant_sounds = relative_count_to_relevant_sounds,\n",
    "            relative_count_to_corpus = relative_count_to_corpus, \n",
    "            distinguish_homophones = distinguish_homophones,\n",
    "            environment_filter = environment_filter,\n",
    "            stop_check = stop_check, call_back = call_back)\n",
    "        results.append(res[0])\n",
    "\n",
    "        if output_filename is not None:\n",
    "            to_output.append((sp, res[1]))\n",
    "    if output_filename is not None:\n",
    "        save_minimal_pairs(output_filename, to_output)\n",
    "    return sum(results)/len(segment_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from corpustools.corpus.io import binary\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/KCH/Desktop/CorpusTools_KCH_fork/CorpusTools/lemurian.corpus\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd(), 'lemurian.corpus')\n",
    "print(path)\n",
    "corpus = binary.load_binary(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corpustools.corpus.classes.lexicon.Corpus"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana\n",
      "ljosa\n",
      "lwafwasu\n",
      "eNifwe\n",
      "uhalu\n",
      "fpemi\n",
      "losudu\n",
      "suNoba\n",
      "luNoba\n",
      "ki\n",
      "mesera\n",
      "si\n",
      "fjaN\n",
      "meresa\n",
      "rwi\n",
      "ri\n",
      "a\n",
      "lwafwapu\n",
      "usalu\n",
      "rodusu\n",
      "heN\n",
      "swafwapu\n",
      "aN\n",
      "asamkyo\n",
      "metesa\n",
      "hkahje\n",
      "nwafwapu\n",
      "fmu\n",
      "alamkyo\n",
      "masesa\n",
      "njoso\n",
      "oN\n",
      "iNifwe\n",
      "ti\n",
      "e\n",
      "swi\n",
      "mesesa\n",
      "rosudu\n",
      "meseta\n",
      "ljoso\n"
     ]
    }
   ],
   "source": [
    "for word in corpus:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word1 = corpus.random_word()\n",
    "word2 = corpus.random_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fmu\n",
      "losudu\n"
     ]
    }
   ],
   "source": [
    "print(word1)\n",
    "print(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "d\n",
      "e\n",
      "f\n",
      "i\n",
      "j\n",
      "k\n",
      "l\n",
      "m\n",
      "n\n",
      "o\n",
      "p\n",
      "r\n",
      "s\n",
      "t\n",
      "u\n",
      "w\n",
      "x\n",
      "y\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "for segment in corpus.inventory:\n",
    "    print(segment.symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "5\n",
      "8\n",
      "6\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "3\n",
      "6\n",
      "2\n",
      "4\n",
      "6\n",
      "3\n",
      "2\n",
      "1\n",
      "8\n",
      "5\n",
      "6\n",
      "3\n",
      "8\n",
      "2\n",
      "7\n",
      "6\n",
      "6\n",
      "8\n",
      "3\n",
      "7\n",
      "6\n",
      "5\n",
      "2\n",
      "6\n",
      "2\n",
      "1\n",
      "3\n",
      "6\n",
      "6\n",
      "6\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for word in corpus: \n",
    "    print(len(word.transcription))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f.m.u"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1.transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corpustools.corpus.classes.lexicon.Transcription"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word1.transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1.frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copy the BaseCorpusContext class from context managers so that you can create a corpus\n",
    "# context to practice with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseCorpusContext(object):\n",
    "    \"\"\"\n",
    "    Abstract Corpus context class that all other contexts inherit from.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : Corpus\n",
    "        Corpus to form context from\n",
    "    sequence_type : str\n",
    "        Sequence type to evaluate algorithms on (i.e., 'transcription')\n",
    "    type_or_token : str\n",
    "        The type of frequency to use for calculations\n",
    "    attribute : Attribute, optional\n",
    "        Attribute to save results to for calculations involving all words\n",
    "        in the Corpus\n",
    "    frequency_threshold: float, optional\n",
    "        If specified, ignore words below this token frequency\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus, sequence_type, type_or_token,\n",
    "                attribute = None, frequency_threshold = 0):\n",
    "        self.sequence_type = sequence_type\n",
    "        self.type_or_token = type_or_token\n",
    "        self.corpus = corpus\n",
    "        self.attribute = attribute\n",
    "        self._freq_base = {}\n",
    "        self.length = None\n",
    "        self.frequency_threshold = frequency_threshold\n",
    "\n",
    "    @property\n",
    "    def inventory(self):\n",
    "        return self.corpus.inventory\n",
    "\n",
    "    @property\n",
    "    def specifier(self):\n",
    "        return self.corpus.specifier\n",
    "\n",
    "    def __enter__(self):\n",
    "        if self.attribute is not None:\n",
    "            self.corpus.add_attribute(self.attribute,initialize_defaults = False)\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.length is not None:\n",
    "            return self.length\n",
    "        else:\n",
    "            counter = 0\n",
    "            for w in self:\n",
    "                counter += 1\n",
    "            self.length = counter\n",
    "            return self.length\n",
    "\n",
    "    def get_frequency_base(self, gramsize = 1, halve_edges = False, probability = False):\n",
    "        \"\"\"\n",
    "        Generate (and cache) frequencies for each segment in the Corpus.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        halve_edges : boolean\n",
    "            If True, word boundary symbols ('#') will only be counted once\n",
    "            per word, rather than twice.  Defaults to False.\n",
    "\n",
    "        gramsize : integer\n",
    "            Size of n-gram to use for getting frequency, defaults to 1 (unigram)\n",
    "\n",
    "        probability : boolean\n",
    "            If True, frequency counts will be normalized by total frequency,\n",
    "            defaults to False\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Keys are segments (or sequences of segments) and values are\n",
    "            their frequency in the Corpus\n",
    "        \"\"\"\n",
    "        if (gramsize) not in self._freq_base:\n",
    "            freq_base = collections.defaultdict(float)\n",
    "            for word in self:\n",
    "                tier = getattr(word, self.sequence_type)\n",
    "                if self.sequence_type == 'spelling':\n",
    "                    seq = ['#'] + [x for x in tier] + ['#']\n",
    "                else:\n",
    "                    seq = tier.with_word_boundaries()\n",
    "                grams = zip(*[seq[i:] for i in range(gramsize)])\n",
    "                for x in grams:\n",
    "                    if len(x) == 1:\n",
    "                        x = x[0]\n",
    "                    freq_base[x] += word.frequency\n",
    "            freq_base['total'] = sum(value for value in freq_base.values())\n",
    "            self._freq_base[(gramsize)] = freq_base\n",
    "        freq_base = self._freq_base[(gramsize)]\n",
    "        return_dict = { k:v for k,v in freq_base.items()}\n",
    "        if halve_edges and '#' in return_dict:\n",
    "            return_dict['#'] = (return_dict['#'] / 2) + 1\n",
    "            if not probability:\n",
    "                return_dict['total'] -= return_dict['#'] - 2\n",
    "        if probability:\n",
    "            return_dict = { k:v/freq_base['total'] for k,v in return_dict.items()}\n",
    "        return return_dict\n",
    "\n",
    "    def get_phone_probs(self, gramsize = 1, probability = True, preserve_position = True, log_count = True):\n",
    "        \"\"\"\n",
    "        Generate (and cache) phonotactic probabilities for segments in\n",
    "        the Corpus.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        gramsize : integer\n",
    "            Size of n-gram to use for getting frequency, defaults to 1 (unigram)\n",
    "\n",
    "        probability : boolean\n",
    "            If True, frequency counts will be normalized by total frequency,\n",
    "            defaults to False\n",
    "\n",
    "        preserve_position : boolean\n",
    "            If True, segments will in different positions in the transcription\n",
    "            will not be collapsed, defaults to True\n",
    "\n",
    "        log_count : boolean\n",
    "            If True, token frequencies will be logrithmically-transformed\n",
    "            prior to being summed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Keys are segments (or sequences of segments) and values are\n",
    "            their phonotactic probability in the Corpus\n",
    "        \"\"\"\n",
    "        if (gramsize, preserve_position, log_count) not in self._freq_base:\n",
    "            freq_base = collections.defaultdict(float)\n",
    "            totals = collections.defaultdict(float)\n",
    "            for word in self:\n",
    "                freq = word.frequency\n",
    "                if self.type_or_token != 'type' and log_count:\n",
    "                    freq = math.log(freq)\n",
    "                grams = zip(*[getattr(word, self.sequence_type)[i:] for i in range(gramsize)])\n",
    "\n",
    "                for i, x in enumerate(grams):\n",
    "                    #if len(x) == 1:\n",
    "                    #    x = x[0]\n",
    "                    if preserve_position:\n",
    "                        x = (x,i)\n",
    "                        totals[i] += freq\n",
    "                    freq_base[x] += freq\n",
    "\n",
    "            if not preserve_position:\n",
    "                freq_base['total'] = sum(value for value in freq_base.values())\n",
    "            else:\n",
    "                freq_base['total'] = totals\n",
    "            self._freq_base[(gramsize, preserve_position, log_count)] = freq_base\n",
    "\n",
    "        freq_base = self._freq_base[(gramsize,preserve_position, log_count)]\n",
    "        return_dict = { k:v for k,v in freq_base.items()}\n",
    "        if probability and not preserve_position:\n",
    "            return_dict = { k:v/freq_base['total'] for k,v in return_dict.items()}\n",
    "        elif probability:\n",
    "            return_dict = { k:v/freq_base['total'][k[1]] for k,v in return_dict.items() if k != 'total'}\n",
    "        return return_dict\n",
    "\n",
    "    def __exit__(self, exc_type, exc, exc_tb):\n",
    "        if exc_type is None:\n",
    "            return True\n",
    "        else:\n",
    "            if self.attribute is not None:\n",
    "                self.corpus.remove_attribute(self.attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CanonicalVariantContext(BaseCorpusContext):\n",
    "    \"\"\"\n",
    "    Corpus context that uses canonical forms for transcriptions and tiers\n",
    "\n",
    "    See the documentation of `BaseCorpusContext` for additional information\n",
    "    \"\"\"\n",
    "    def __exit__(self, exc_type, exc, exc_tb):\n",
    "        BaseCorpusContext.__exit__(self, exc_type, exc, exc_tb)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for word in self.corpus:\n",
    "            if math.isnan(word.frequency):\n",
    "                continue\n",
    "            if self.type_or_token == 'token' and word.frequency == 0:\n",
    "                continue\n",
    "            if self.frequency_threshold > 0 and word.frequency < self.frequency_threshold:\n",
    "                continue\n",
    "            w = copy.copy(word)\n",
    "            if self.type_or_token == 'type':\n",
    "                w.frequency = 1\n",
    "            w.original = word\n",
    "            yield w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_context = CanonicalVariantContext(corpus, \"transcription\", \"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CanonicalVariantContext"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transcription'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_context.sequence_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_minpair(word1, word2, corpus_context, [('p','t')], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "luNoba\n"
     ]
    }
   ],
   "source": [
    "word1 = corpus.words[17]\n",
    "print(word1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suNoba\n"
     ]
    }
   ],
   "source": [
    "word2 = corpus.words[34]\n",
    "print(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'transcription'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-31536fc2d639>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mis_minpair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m't'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-3f433eaff852>\u001b[0m in \u001b[0;36mis_minpair\u001b[0;34m(first, second, corpus_context, segment_pairs, environment_filter)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mone\u001b[0m \u001b[0mdifference\u001b[0m \u001b[0mbetween\u001b[0m \u001b[0mfirst\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Q: how do I find out what .sequence_type does?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# in contextmanagers.py: sequence_type is a string that is defined as \"Sequence type to evaluate algorithms on (i.e., 'transcription')\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# so this is saying get the attribute labeled in corpus_context.sequence_type (\"transcription\" or \"vowels\" or whatever) of the item named 'first'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'transcription'"
     ]
    }
   ],
   "source": [
    "is_minpair(word1, word2, corpus_context, [('p','t')], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word1) # oh for pete's sake...how do I keep it as a word object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I'm sure there's a better way to do this, but for the sake of just having ANYTHING to work with:\n",
    "for w in corpus:\n",
    "    if str(w) == 'luNoba':\n",
    "        word1 = w\n",
    "    elif str(w) == 'suNoba':\n",
    "        word2 = w\n",
    "    else:\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Word: 'luNoba'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Word: 'suNoba'>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_minpair(word1, word2, corpus_context, [('p','t')], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_minpair(word1, word2, corpus_context, [('l','s')], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Yay, ok, is_minpair is working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05,\n",
       " [((<Word: 'luNoba'>, l.u.n.o.b.a), (<Word: 'suNoba'>, s.u.n.o.b.a)),\n",
       "  ((<Word: 'lwafwapu'>, l.w.a.f.w.a.p.u),\n",
       "   (<Word: 'swafwapu'>, s.w.a.f.w.a.p.u))])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minpair_fl(corpus_context, [('s','l')],\n",
    "        relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = True, distinguish_homophones = False,\n",
    "        environment_filter = None,\n",
    "        stop_check = None, call_back = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15384615384615385,\n",
       " [((<Word: 'luNoba'>, l.u.n.o.b.a), (<Word: 'suNoba'>, s.u.n.o.b.a)),\n",
       "  ((<Word: 'lwafwapu'>, l.w.a.f.w.a.p.u),\n",
       "   (<Word: 'swafwapu'>, s.w.a.f.w.a.p.u))])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minpair_fl(corpus_context, [('s','l')],\n",
    "        relative_count_to_relevant_sounds = True, relative_count_to_whole_corpus = False, distinguish_homophones = False,\n",
    "        environment_filter = None,\n",
    "        stop_check = None, call_back = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note these literally are number of MPs divided by the number of words {in the corpus, \n",
    "# containing the sounds}, rather than e.g. the number of WORDS involved in the MPs.\n",
    "# so e.g. if you have 2 minimal pairs out of a corpus of 40 words, the relative min pair count\n",
    "# to the corpus is 2/40 = 0.05; you would have to double this to get the number\n",
    "# of words in the corpus involved in minimal pairs (4 / 40 = 0.1, or .05 * 2 = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_pairs = [('s','l')]\n",
    "type(segment_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ana\n",
      "ljosa\n",
      "lwafwasu\n",
      "eNifwe\n",
      "uhalu\n",
      "fpemi\n",
      "losudu\n",
      "suNoba\n",
      "luNoba\n",
      "ki\n",
      "mesera\n",
      "si\n",
      "fjaN\n",
      "meresa\n",
      "rwi\n",
      "ri\n",
      "a\n",
      "lwafwapu\n",
      "usalu\n",
      "rodusu\n",
      "heN\n",
      "swafwapu\n",
      "aN\n",
      "asamkyo\n",
      "metesa\n",
      "hkahje\n",
      "nwafwapu\n",
      "fmu\n",
      "alamkyo\n",
      "masesa\n",
      "njoso\n",
      "oN\n",
      "iNifwe\n",
      "ti\n",
      "e\n",
      "swi\n",
      "mesesa\n",
      "rosudu\n",
      "meseta\n",
      "ljoso\n"
     ]
    }
   ],
   "source": [
    "for w in corpus_context:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_context.corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# updated the doc strings on this one\n",
    "# this one also now has two different \"relative count\" options\n",
    "def all_pairwise_fls(corpus_context, relative_fl = False,\n",
    "                    algorithm = 'minpair',\n",
    "                    relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = True, distinguish_homophones = False,\n",
    "                    environment_filter = None):\n",
    "    \"\"\"Calculate the functional load of the contrast between two segments as a count of minimal pairs.\n",
    "    This version calculates the functional load for ALL pairs of segments in the inventory,\n",
    "    which could be useful for visually mapping out phoneme inventories.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus_context : CorpusContext\n",
    "        Context manager for a corpus\n",
    "    relative_fl : bool\n",
    "        If False, return the FL for all segment pairs. If True, return\n",
    "        the relative (average) FL for each segment.\n",
    "    algorithm : str {'minpair', 'deltah'}\n",
    "        Algorithm to use for calculating functional load: \"minpair\" for\n",
    "        minimal pair count or \"deltah\" for change in entropy.\n",
    "    relative_count_to_relevant_sounds : bool, optional\n",
    "        If True, divide the number of minimal pairs by the total count\n",
    "        by the total number of words that contain either of the two segments.\n",
    "    relative_count_to_whole_corpus : bool, optional\n",
    "        If True, divide the number of minimal pairs by the total number of words \n",
    "        in the corpus (regardless of whether those words contain the target sounds).\n",
    "        Defaults to True.\n",
    "    distinguish_homophones : bool, optional\n",
    "        If False, then you'll count sock~shock (sock=clothing) and\n",
    "        sock~shock (sock=punch) as just one minimal pair; but if True,\n",
    "        you'll overcount alternative spellings of the same word, e.g.\n",
    "        axel~actual and axle~actual. False is the value used by Wedel et al.\n",
    "    environment_filter : EnvironmentFilter\n",
    "        Allows the user to restrict the neutralization process to segments in\n",
    "        particular segmental contexts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of tuple(tuple(str, st), float)\n",
    "    OR\n",
    "    list of (str, float)\n",
    "        Normally returns a list of all Segment pairs and their respective functional load values, as length-2 tuples ordered by FL.\n",
    "        If calculating relative FL (i.e., average FL for a segment), returns a dictionary of each segment and its relative (average) FL, with entries ordered by FL.\n",
    "    \"\"\"\n",
    "    fls = {}\n",
    "    total_calculations = ((((len(corpus_context.inventory)-1)**2)-len(corpus_context.inventory)-1)/2)+1\n",
    "    ct = 1\n",
    "    t = time.time()\n",
    "    if '' in corpus_context.inventory:\n",
    "        raise Exception('Warning: Calculation of functional load for all segment pairs requires that all items in corpus have a non-null transcription.')\n",
    "    \n",
    "    # Count the number of words in the corpus (needed if relative_count_to_whole_corpus is True)\n",
    "    num_words_in_corpus = len(corpus_context.corpus)\n",
    "    \n",
    "    for i, s1 in enumerate(corpus_context.inventory[:-1]):\n",
    "        for s2 in corpus_context.inventory[i+1:]:\n",
    "            if s1 != '#' and s2 != '#':\n",
    "                print('Performing FL calculation {} out of {} possible'.format(str(ct), str(total_calculations)))\n",
    "                ct += 1\n",
    "                print('Duration of last calculation: {}'.format(str(time.time() - t)))\n",
    "                t = time.time()\n",
    "                if type(s1) != str:\n",
    "                    s1 = s1.symbol\n",
    "                if type(s2) != str:\n",
    "                    s2 = s2.symbol\n",
    "                if algorithm == 'minpair':\n",
    "                    fl = minpair_fl(corpus_context, [(s1, s2)],\n",
    "                            relative_count_to_relevant_sounds=relative_count_to_relevant_sounds,\n",
    "                            relative_count_to_whole_corpus=relative_count_to_whole_corpus,\n",
    "                            distinguish_homophones=distinguish_homophones,\n",
    "                            environment_filter=environment_filter)[0]\n",
    "                elif algorithm == 'deltah':\n",
    "                    fl = deltah_fl(corpus_context, [(s1, s2)],\n",
    "                    environment_filter=environment_filter)\n",
    "                fls[(s1, s2)] = fl\n",
    "    if not relative_fl:\n",
    "        ordered_fls = sorted([(pair, fls[pair]) for pair in fls], key=lambda p: p[1], reverse=True)\n",
    "        return ordered_fls\n",
    "    elif relative_fl:\n",
    "        rel_fls = {}\n",
    "        for s in corpus_context.inventory:\n",
    "            if type(s) != str:\n",
    "                s = s.symbol\n",
    "            if s != '#':\n",
    "                total = 0.0\n",
    "                for pair in fls:\n",
    "                    if s == pair[0] or s == pair[1]:\n",
    "                        total += fls[pair]\n",
    "                rel_fls[s] = total / (len(corpus_context.inventory) - 1)\n",
    "        ordered_rel_fls = sorted([(s, rel_fls[s]) for s in rel_fls], key=lambda p: p[1], reverse=True)\n",
    "        return ordered_rel_fls\n",
    "\n",
    "    \n",
    "    ## Filter out words that have none of the target segments\n",
    "    ## (for relative_count_to_relevant_sounds as well as improving runtime)\n",
    "    contain_target_segment = []\n",
    "    if call_back is not None:\n",
    "        call_back('Finding words with the specified segments...')\n",
    "        call_back(0, len(corpus_context))\n",
    "        cur = 0\n",
    "\n",
    "    all_target_segments = list(itertools.chain.from_iterable(segment_pairs)) # creates a list of target segments from the list of tuples\n",
    "    for w in corpus_context: # loops through the words in the context manager\n",
    "        if stop_check is not None and stop_check():\n",
    "            return\n",
    "        if call_back is not None:\n",
    "            cur += 1\n",
    "            if cur % 100 == 0:\n",
    "                call_back(cur)\n",
    "        tier = getattr(w, corpus_context.sequence_type)\n",
    "        if any([s in tier for s in all_target_segments]):\n",
    "                contain_target_segment.append(w)\n",
    "    if stop_check is not None and stop_check():\n",
    "        return\n",
    "\n",
    "    ## Find minimal pairs\n",
    "    minpairs = []\n",
    "    if call_back is not None:\n",
    "        call_back('Finding minimal pairs...')\n",
    "        if len(contain_target_segment) >= 2:\n",
    "            call_back(0,factorial(len(contain_target_segment))/(factorial(len(contain_target_segment)-2)*2))\n",
    "        cur = 0\n",
    "    for first, second in itertools.combinations(contain_target_segment, 2):\n",
    "        if stop_check is not None and stop_check():\n",
    "            return\n",
    "        if call_back is not None:\n",
    "            cur += 1\n",
    "            if cur % 100 == 0:\n",
    "                call_back(cur)\n",
    "        if is_minpair(first, second, corpus_context, segment_pairs, environment_filter):\n",
    "            ordered_pair = sorted([(first, getattr(first, corpus_context.sequence_type)),\n",
    "                                   (second, getattr(second, corpus_context.sequence_type))],\n",
    "                                   key = lambda x: x[1]) # sort by tier/transcription\n",
    "            minpairs.append(tuple(ordered_pair))\n",
    "\n",
    "    ## Generate output #### CHECK THIS!\n",
    "    if not distinguish_homophones:\n",
    "        actual_minpairs = {}\n",
    "\n",
    "        for pair in minpairs:\n",
    "            if stop_check is not None and stop_check():\n",
    "                return\n",
    "            key = (pair[0][1], pair[1][1]) # Keys are tuples of transcriptions\n",
    "            if key not in actual_minpairs:\n",
    "                actual_minpairs[key] = (pair[0][0], pair[1][0]) # Values are words\n",
    "            else:\n",
    "                pair_freq = pair[0][0].frequency + pair[1][0].frequency\n",
    "                existing_freq = actual_minpairs[key][0].frequency + \\\n",
    "                                actual_minpairs[key][1].frequency\n",
    "                if pair_freq > existing_freq:\n",
    "                    actual_minpairs[key] = (pair[0][0], pair[1][0])\n",
    "        result = sum((x[0].frequency + x[1].frequency)/2\n",
    "                    for x in actual_minpairs.values())\n",
    "    else:\n",
    "        result = sum((x[0][0].frequency + x[1][0].frequency)/2 for x in minpairs)\n",
    "\n",
    "    if relative_count_to_relevant_sounds and len(contain_target_segment) > 0:\n",
    "        result /= sum(x.frequency for x in contain_target_segment)\n",
    "\n",
    "    # added what to do if the relative count to the whole corpus is true, namely, divide by the number of words in the corpus    \n",
    "    elif relative_count_to_whole_corpus:\n",
    "        result = result / num_words_in_corpus\n",
    "    \n",
    "    return (result, minpairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.0, [((<Word: 'luNoba'>, l.u.n.o.b.a), (<Word: 'suNoba'>, s.u.n.o.b.a)), ((<Word: 'lwafwapu'>, l.w.a.f.w.a.p.u), (<Word: 'swafwapu'>, s.w.a.f.w.a.p.u))])\n"
     ]
    }
   ],
   "source": [
    "raw_s_l = minpair_fl(corpus_context, [('s','l')],\n",
    "        relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = False, distinguish_homophones = False,\n",
    "        environment_filter = None,\n",
    "        stop_check = None, call_back = None)\n",
    "print(raw_s_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_s_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_s_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<Word: 'lwafwapu'>, l.w.a.f.w.a.p.u), (<Word: 'swafwapu'>, s.w.a.f.w.a.p.u))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_s_l[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of minimal pairs based on [s]/[l], relative to the size of the corpus, is: \n",
      "(0.05, [((<Word: 'luNoba'>, l.u.n.o.b.a), (<Word: 'suNoba'>, s.u.n.o.b.a)), ((<Word: 'lwafwapu'>, l.w.a.f.w.a.p.u), (<Word: 'swafwapu'>, s.w.a.f.w.a.p.u))])\n"
     ]
    }
   ],
   "source": [
    "rel_s_l_to_corpus = minpair_fl(corpus_context, [('s','l')],\n",
    "        relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = True, distinguish_homophones = False,\n",
    "        environment_filter = None,\n",
    "        stop_check = None, call_back = None)\n",
    "\n",
    "print(\"The number of minimal pairs based on [s]/[l], relative to the size of the corpus, is: \")\n",
    "print(rel_s_l_to_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of minimal pairs based on [s]/[l], relative to the number of words containing [s] and [l] in the corpus, is: \n",
      "0.15384615384615385\n"
     ]
    }
   ],
   "source": [
    "rel_s_l_to_relevant_words = minpair_fl(corpus_context, [('s','l')],\n",
    "        relative_count_to_relevant_sounds = True, relative_count_to_whole_corpus = False, distinguish_homophones = False,\n",
    "        environment_filter = None,\n",
    "        stop_check = None, call_back = None)\n",
    "\n",
    "print(\"The number of minimal pairs based on [s]/[l], relative to the number of words containing [s] and [l] in the corpus, is: \")\n",
    "print(rel_s_l_to_relevant_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
