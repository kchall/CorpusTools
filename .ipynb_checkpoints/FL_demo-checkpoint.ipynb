{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global import statements\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from math import *\n",
    "import itertools\n",
    "import copy\n",
    "from math import factorial\n",
    "import time\n",
    "import math # This is added because it is called later on in the script.\n",
    "\n",
    "# relative import statements\n",
    "# these will only work if the FL script (or the script that calls it) is in the uppermost CorpusTools folder\n",
    "from corpustools.exceptions import FuncLoadError \n",
    "from corpustools.funcload.io import save_minimal_pairs \n",
    "from corpustools.corpus.classes.lexicon import EnvironmentFilter\n",
    "\n",
    "# function definitions\n",
    "\n",
    "# I updated the doc strings here:\n",
    "def is_minpair(first, second, corpus_context, segment_pairs, environment_filter): \n",
    "    \"\"\"Return True iff first/second are a minimal pair.\n",
    "    Checks that all segments in those words are identical OR a valid segment pair\n",
    "    (from segment_pairs) and fit the environment_filter, and that there is at least\n",
    "    one difference between first and second.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    first, second: Word\n",
    "        Two words to evaluate as a minimal pair. \n",
    "    corpus_context: CorpusContext\n",
    "        The context manager that the corpus should be evaluated from (e.g., canonical variants).\n",
    "    segment_pairs: List\n",
    "        list of length-2 tuples of str\n",
    "    environment_filter: Environment\n",
    "        The environment in which words should be evaluated for being a minimal pair.\n",
    "    \n",
    "    \"\"\"\n",
    "    first = getattr(first, corpus_context.sequence_type)\n",
    "    second = getattr(second, corpus_context.sequence_type)\n",
    "    if len(first) != len(second):\n",
    "        return False\n",
    "    has_difference = False\n",
    "    for i in range(len(first)):\n",
    "        if first[i] == second[i]:\n",
    "            continue\n",
    "        elif (conflateable(first[i], second[i], segment_pairs) \n",
    "            and fits_environment(first, second, i, environment_filter)): \n",
    "            has_difference = True\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    if has_difference:\n",
    "        return True\n",
    "\n",
    "\n",
    "# I updated the doc strings here:\n",
    "def conflateable(seg1, seg2, segment_pairs):\n",
    "    \"\"\"Return True iff seg1 and seg2 are exactly one of the segment pairs\n",
    "    in segment_pairs (ignoring ordering of either).\n",
    "\n",
    "    seg1 and seg2 will never be identical in the input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    seg1, seg2: Segment\n",
    "        Two segments on which minimal pairs might hinge. \n",
    "    segment_pairs: List\n",
    "        list of length-2 tuples of str\n",
    "    \n",
    "    \"\"\"\n",
    "    for segment_pair in segment_pairs:\n",
    "        seg_set = set(segment_pair)\n",
    "        if seg1 in seg_set and seg2 in seg_set:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# I didn't change anything in the environment-filter functions:\n",
    "def fits_environment(w1, w2, index, environment_filter):\n",
    "    \"\"\"Return True iff for both w1 and w2 (tiers), the environment\n",
    "    of its i'th element passes the environment_filter.\n",
    "\n",
    "    \"\"\"\n",
    "    if not environment_filter:\n",
    "        return True\n",
    "\n",
    "    def ready_for_re(word, index):\n",
    "        w = [str(seg) for seg in word]\n",
    "        w[index] = '_'\n",
    "        return ' '.join(w)\n",
    "\n",
    "    w1 = ready_for_re(w1, index)\n",
    "    w2 = ready_for_re(w2, index)\n",
    "    env_re = make_environment_re(environment_filter) \n",
    "\n",
    "    return (bool(re.search(env_re, w1)) and bool(re.search(env_re, w2)))\n",
    "\n",
    "\n",
    "# I didn't change anything in the environment-filter functions:\n",
    "def ready_for_re(word, index):\n",
    "        w = [str(seg) for seg in word]\n",
    "        w[index] = '_'\n",
    "        return ' '.join(w)\n",
    "\n",
    "\n",
    "# I didn't change anything in the environment-filter functions:\n",
    "def make_environment_re(environment_filter): \n",
    "    if environment_filter.lhs:\n",
    "        re_lhs = ' '.join(['('+('|'.join([seg for seg in position])+')') for position in environment_filter.lhs])\n",
    "        re_lhs = re_lhs.replace('#', '^')\n",
    "    else:\n",
    "        re_lhs = ''\n",
    "\n",
    "    if environment_filter.rhs:\n",
    "        re_rhs = ' '.join(['('+('|'.join([seg for seg in position])+')') for position in environment_filter.rhs])\n",
    "        re_rhs = re_rhs.replace('#', '$')\n",
    "    else:\n",
    "        re_rhs = ''\n",
    "\n",
    "    if re_lhs and not re_lhs.endswith('^)'):\n",
    "        re_lhs += ' '\n",
    "    if re_rhs and not re_rhs.endswith('($'):\n",
    "        re_rhs = ' ' + re_rhs\n",
    "    return re_lhs + '_' + re_rhs\n",
    "\n",
    "\n",
    "# This is the function I really edited\n",
    "# I changed the parameter called 'relative_count' to 'relative_count_to_relevant_sounds' and changed its default value.\n",
    "# I added a new parameter, 'relative_count_to_whole_corpus', and set its default to true.\n",
    "# I updated the doc strings.\n",
    "def minpair_fl(corpus_context, segment_pairs,\n",
    "        relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = True, distinguish_homophones = False,\n",
    "        environment_filter = None,\n",
    "        stop_check = None, call_back = None):\n",
    "    \"\"\"Calculate the functional load of the contrast between two segments\n",
    "    as a count of minimal pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus_context : CorpusContext\n",
    "        Context manager for a corpus\n",
    "    segment_pairs : list of length-2 tuples of str\n",
    "        The pairs of segments to be conflated.\n",
    "    relative_count_to_relevant_sounds : bool, optional\n",
    "        If True, divide the number of minimal pairs by \n",
    "        by the total number of words that contain either of the two segments.\n",
    "        # changed the name of this from \"relative_count\" to \"relative_count_to_relevant_sounds\"\n",
    "        # set its default to False above\n",
    "    relative_count_to_whole_corpus : bool, optional\n",
    "        If True, divide the number of minimal pairs by the total number of words \n",
    "        in the corpus (regardless of whether those words contain the target sounds).\n",
    "        Defaults to True.\n",
    "    distinguish_homophones : bool, optional\n",
    "        If False, then you'll count sock~shock (sock=clothing) and\n",
    "        sock~shock (sock=punch) as just one minimal pair; but if True,\n",
    "        you'll overcount alternative spellings of the same word, e.g.\n",
    "        axel~actual and axle~actual. False is the value used by Wedel et al.\n",
    "    environment_filter : EnvironmentFilter\n",
    "        Allows the user to restrict the neutralization process to segments in\n",
    "        particular segmental contexts\n",
    "    stop_check : callable, optional\n",
    "        Optional function to check whether to gracefully terminate early\n",
    "    call_back : callable, optional\n",
    "        Optional function to supply progress information during the function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple(int or float, list)\n",
    "        Tuple of: 0. if `relative_count_to_relevant_sounds`==False and \n",
    "        `relative_count_to_whole_corpus`==False, an int of the raw number of\n",
    "        minimal pairs; if `relative_count_to_relevant_sounds`==True, a float of that\n",
    "        count divided by the total number of words in the corpus that\n",
    "        include either `s1` or `s2`; if `relative_count_to_whole_corpus`==True, a\n",
    "        float of the raw count divided by the total number of words in the corpus; \n",
    "        and 1. list of minimal pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    if stop_check is not None and stop_check():\n",
    "        return\n",
    "\n",
    "    ## Count the number of words in the corpus (needed if relative_count_to_whole_corpus is True)\n",
    "    num_words_in_corpus = len(corpus_context.corpus)\n",
    "\n",
    "        ## Filter out words that have none of the target segments\n",
    "    ## (for relative_count_to_relevant_sounds as well as improving runtime)\n",
    "    contain_target_segment = []\n",
    "    if call_back is not None:\n",
    "        call_back('Finding words with the specified segments...')\n",
    "        call_back(0, len(corpus_context))\n",
    "        cur = 0\n",
    "\n",
    "    all_target_segments = list(itertools.chain.from_iterable(segment_pairs)) # creates a list of target segments from the list of tuples\n",
    "    for w in corpus_context: # loops through the words in the context manager?\n",
    "        if stop_check is not None and stop_check():\n",
    "            return\n",
    "        if call_back is not None:\n",
    "            cur += 1\n",
    "            if cur % 100 == 0:\n",
    "                call_back(cur)\n",
    "        tier = getattr(w, corpus_context.sequence_type)\n",
    "        if any([s in tier for s in all_target_segments]):\n",
    "                contain_target_segment.append(w)\n",
    "    if stop_check is not None and stop_check():\n",
    "        return\n",
    "\n",
    "    ## Find minimal pairs\n",
    "    minpairs = []\n",
    "    if call_back is not None:\n",
    "        call_back('Finding minimal pairs...')\n",
    "        if len(contain_target_segment) >= 2:\n",
    "            call_back(0,factorial(len(contain_target_segment))/(factorial(len(contain_target_segment)-2)*2))\n",
    "        cur = 0\n",
    "    for first, second in itertools.combinations(contain_target_segment, 2):\n",
    "        if stop_check is not None and stop_check():\n",
    "            return\n",
    "        if call_back is not None:\n",
    "            cur += 1\n",
    "            if cur % 100 == 0:\n",
    "                call_back(cur)\n",
    "        if is_minpair(first, second, corpus_context, segment_pairs, environment_filter):\n",
    "            ordered_pair = sorted([(first, getattr(first, corpus_context.sequence_type)),\n",
    "                                   (second, getattr(second, corpus_context.sequence_type))],\n",
    "                                   key = lambda x: x[1]) # sort by tier/transcription\n",
    "            minpairs.append(tuple(ordered_pair))\n",
    "\n",
    "    ## Generate output \n",
    "    if not distinguish_homophones:\n",
    "        actual_minpairs = {}\n",
    "\n",
    "        for pair in minpairs:\n",
    "            if stop_check is not None and stop_check():\n",
    "                return\n",
    "            key = (pair[0][1], pair[1][1]) # Keys are tuples of transcriptions\n",
    "            if key not in actual_minpairs:\n",
    "                actual_minpairs[key] = (pair[0][0], pair[1][0]) # Values are words\n",
    "            else:\n",
    "                pair_freq = pair[0][0].frequency + pair[1][0].frequency\n",
    "                existing_freq = actual_minpairs[key][0].frequency + \\\n",
    "                                actual_minpairs[key][1].frequency\n",
    "                if pair_freq > existing_freq:\n",
    "                    actual_minpairs[key] = (pair[0][0], pair[1][0])\n",
    "        result = sum((x[0].frequency + x[1].frequency)/2\n",
    "                    for x in actual_minpairs.values())\n",
    "    else:\n",
    "        result = sum((x[0][0].frequency + x[1][0].frequency)/2 for x in minpairs)\n",
    "\n",
    "    if relative_count_to_relevant_sounds and len(contain_target_segment) > 0:\n",
    "        result /= sum(x.frequency for x in contain_target_segment)\n",
    "        \n",
    "        # Q: do I need to put anything here between these if statements (like make it an else if)? or can I just have them back to back?\n",
    "        # relative_count_to_relevant_sounds and relative_count_to_whole_corpus won't both be true\n",
    "        # but both COULD be false, in which case, result should just stay as the raw count \n",
    "        # that I assume is returned above -- elif will ensure that only one runs; if will allow both to run\n",
    "        \n",
    "    elif relative_count_to_whole_corpus:\n",
    "        result = result / num_words_in_corpus\n",
    "    \n",
    "    return (result, minpairs)\n",
    "\n",
    "\n",
    "# I didn't change the delta-H functions.\n",
    "def deltah_fl(corpus_context, segment_pairs, environment_filter = None,\n",
    "            stop_check = None, call_back = None):\n",
    "    \"\"\"Calculate the functional load of the contrast between between two\n",
    "    segments as the decrease in corpus entropy caused by a merger.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus_context : CorpusContext\n",
    "        Context manager for a corpus\n",
    "    segment_pairs : list of length-2 tuples of str\n",
    "        The pairs of segments to be conflated.\n",
    "    environment_filter : EnvironmentFilter\n",
    "        Allows the user to restrict the neutralization process to segments in\n",
    "        particular segmental contexts\n",
    "    stop_check : callable, optional\n",
    "        Optional function to check whether to gracefully terminate early\n",
    "    call_back : callable, optional\n",
    "        Optional function to supply progress information during the function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The difference between a) the entropy of the choice among\n",
    "        non-homophonous words in the corpus before a merger of `s1`\n",
    "        and `s2` and b) the entropy of that choice after the merger.\n",
    "    \"\"\"\n",
    "    if call_back is not None:\n",
    "        call_back('Finding instances of segments...')\n",
    "        call_back(0, len(corpus_context))\n",
    "        cur = 0\n",
    "    freq_sum = 0\n",
    "    original_probs = defaultdict(float)\n",
    "\n",
    "    all_target_segments = list(itertools.chain.from_iterable(segment_pairs))\n",
    "    if environment_filter:\n",
    "        filled_environment = EnvironmentFilter(tuple(all_target_segments),\n",
    "                                               environment_filter.lhs,\n",
    "                                               environment_filter.rhs)\n",
    "\n",
    "    for w in corpus_context:\n",
    "        if stop_check is not None and stop_check():\n",
    "            return\n",
    "        if call_back is not None:\n",
    "            cur += 1\n",
    "            if cur % 20 == 0:\n",
    "                call_back(cur)\n",
    "\n",
    "        f = w.frequency\n",
    "\n",
    "        original_probs[getattr(w, corpus_context.sequence_type)] += f\n",
    "        freq_sum += f\n",
    "\n",
    "    original_probs = {k:v/freq_sum for k,v in original_probs.items()}\n",
    "\n",
    "    if stop_check is not None and stop_check():\n",
    "        return\n",
    "    preneutr_h = entropy(original_probs.values())\n",
    "\n",
    "    neutralized_probs = defaultdict(float)\n",
    "    if call_back is not None:\n",
    "        call_back('Neutralizing instances of segments...')\n",
    "        call_back(0, len(list(original_probs.keys())))\n",
    "        cur = 0\n",
    "    for k,v in original_probs.items():\n",
    "        if stop_check is not None and stop_check():\n",
    "            return\n",
    "        if call_back is not None:\n",
    "            cur += 1\n",
    "            if cur % 100 == 0:\n",
    "                call_back(cur)\n",
    "        if not environment_filter or k.find(filled_environment):\n",
    "            n = [neutralize_segment(seg, segment_pairs)\n",
    "                    for seg in k]\n",
    "            neutralized_probs['.'.join(n)] += v\n",
    "    postneutr_h = entropy(neutralized_probs.values())\n",
    "\n",
    "    if stop_check is not None and stop_check():\n",
    "        return\n",
    "    result = preneutr_h - postneutr_h\n",
    "    if result < 1e-10:\n",
    "        result = 0.0\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# This function is weirdly named. It should really be something like\n",
    "# average_minpair_fl\n",
    "# It has also been changed so as to have two \"relativizer\" options:\n",
    "# one to words containing the relevant segments and one to all\n",
    "# words in the corpus (though it basically does the calculation\n",
    "# by calling the above minpair_fl() function).\n",
    "def relative_minpair_fl(corpus_context, segment,\n",
    "            relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = True, \n",
    "            distinguish_homophones = False, output_filename = None, environment_filter = None,\n",
    "            stop_check = None, call_back = None):\n",
    "    \"\"\"Calculate the average functional load of the contrasts between a\n",
    "    segment and all other segments, as a count of minimal pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus_context : CorpusContext\n",
    "        Context manager for a corpus\n",
    "    segment : str\n",
    "        The target segment.\n",
    "    relative_count_to_relevant_sounds : bool, optional\n",
    "        If True, divide the number of minimal pairs\n",
    "        by the total number of words that contain either of the two segments.\n",
    "    relative_count_to_whole_corpus : bool, optional\n",
    "        If True, divide the number of minimal pairs by the total number of words \n",
    "        in the corpus (regardless of whether those words contain the target sounds).\n",
    "        Defaults to True.\n",
    "    distinguish_homophones : bool, optional\n",
    "        If False, then you'll count sock~shock (sock=clothing) and\n",
    "        sock~shock (sock=punch) as just one minimal pair; but if True,\n",
    "        you'll overcount alternative spellings of the same word, e.g.\n",
    "        axel~actual and axle~actual. False is the value used by Wedel et al.\n",
    "    environment_filter : EnvironmentFilter\n",
    "        Allows the user to restrict the neutralization process to segments in\n",
    "        particular segmental contexts\n",
    "    stop_check : callable, optional\n",
    "        Optional function to check whether to gracefully terminate early\n",
    "    call_back : callable, optional\n",
    "        Optional function to supply progress information during the function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int or float\n",
    "        If `relative_count_to_relevant_sounds`==False and `relative_count_to_whole_corpus`==False,\n",
    "        returns an int of the raw number of\n",
    "        minimal pairs. If `relative_count_to_relevant_sounds`==True, returns a float of\n",
    "        that count divided by the total number of words in the corpus\n",
    "        that include either `s1` or `s2`. If `relative_count_to_whole_corpus`==True, a\n",
    "        float of the raw count divided by the total number of words in the corpus. \n",
    "    \"\"\"\n",
    "    all_segments = corpus_context.inventory\n",
    "    segment_pairs = [(segment,other.symbol) for other in all_segments\n",
    "                        if other.symbol != segment and other.symbol != '#']\n",
    "\n",
    "    results = []\n",
    "    to_output = []\n",
    "    for sp in segment_pairs:\n",
    "        res = minpair_fl(corpus_context, [sp],\n",
    "            relative_count_to_relevant_sounds = relative_count_to_relevant_sounds,\n",
    "            relative_count_to_whole_corpus = relative_count_to_whole_corpus, \n",
    "            distinguish_homophones = distinguish_homophones,\n",
    "            environment_filter = environment_filter,\n",
    "            stop_check = stop_check, call_back = call_back)\n",
    "        results.append(res[0])\n",
    "\n",
    "        if output_filename is not None:\n",
    "            to_output.append((sp, res[1]))\n",
    "    if output_filename is not None:\n",
    "        save_minimal_pairs(output_filename, to_output)\n",
    "    return sum(results)/len(segment_pairs)\n",
    "\n",
    "\n",
    "# I didn't change the delta-H functions.\n",
    "def relative_deltah_fl(corpus_context, segment,\n",
    "                environment_filter = None,\n",
    "                stop_check = None, call_back = None):\n",
    "    \"\"\"Calculate the average functional load of the contrasts between a\n",
    "    segment and all other segments, as the decrease in corpus entropy\n",
    "    caused by a merger.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus_context : CorpusContext\n",
    "        Context manager for a corpus\n",
    "    segment : str\n",
    "        The target segment.\n",
    "    stop_check : callable, optional\n",
    "        Optional function to check whether to gracefully terminate early\n",
    "    call_back : callable, optional\n",
    "        Optional function to supply progress information during the function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The difference between a) the entropy of the choice among\n",
    "        non-homophonous words in the corpus before a merger of `s1`\n",
    "        and `s2` and b) the entropy of that choice after the merger.\n",
    "    \"\"\"\n",
    "    all_segments = corpus_context.inventory\n",
    "    segment_pairs = [(segment,other.symbol) for other in all_segments\n",
    "                        if other.symbol != segment and other.symbol != '#']\n",
    "\n",
    "    results = []\n",
    "    for sp in segment_pairs:\n",
    "        results.append(deltah_fl(corpus_context, [sp],\n",
    "                environment_filter=environment_filter,\n",
    "                stop_check = stop_check, call_back = call_back))\n",
    "    return sum(results)/len(segment_pairs)\n",
    "\n",
    "\n",
    "# I didn't change this.\n",
    "def collapse_segpairs_fl(corpus_context, **kwargs):\n",
    "    func_type = kwargs.get('func_type')\n",
    "    segment_pairs = kwargs.get('segment_pairs')\n",
    "    relative_count = kwargs.get('relative_count')\n",
    "    distinguish_homophones = kwargs.get('distinguish_homophones')\n",
    "    if func_type == 'min_pairs':\n",
    "        fl = minpair_fl(corpus_context, segment_pairs,\n",
    "                        relative_count, distinguish_homophones,\n",
    "                          environment_filter=environment_filter)\n",
    "    elif func_type == 'entropy':\n",
    "        fl = deltah_fl(corpus_context, segment_pairs,\n",
    "          environment_filter=environment_filter)\n",
    "\n",
    "\n",
    "# I didn't change this.\n",
    "def individual_segpairs_fl(corpus_context, **kwargs):\n",
    "    func_type = kwargs.get('func_type')\n",
    "    segment_pairs = kwargs.get('segment_pairs')\n",
    "    relative_count = kwargs.get('relative_count')\n",
    "    distinguish_homophones = kwargs.get('distinguish_homophones')\n",
    "\n",
    "    results = []\n",
    "    for pair in segment_pairs:\n",
    "        if func_type == 'min_pairs':\n",
    "            fl = minpair_fl(corpus_context, [pair],\n",
    "                            relative_count, distinguish_homophones,\n",
    "                              environment_filter=environment_filter)\n",
    "        elif func_type == 'entropy':\n",
    "            fl = deltah_fl(corpus_context, [pair],\n",
    "              environment_filter=environment_filter)\n",
    "        results.append(fl)\n",
    "\n",
    "\n",
    "# I didn't change this.\n",
    "def entropy(probabilities):\n",
    "    \"\"\"Calculate the entropy of a choice from the provided probability distribution.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    probabilities : list of floats\n",
    "        Contains the probability of each item in the list.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Entropy\n",
    "    \"\"\"\n",
    "    return -(sum([p*log(p,2) if p > 0 else 0 for p in probabilities]))\n",
    "\n",
    "\n",
    "# I didn't change this.\n",
    "def neutralize_segment(segment, segment_pairs):\n",
    "    for sp in segment_pairs:\n",
    "        try:\n",
    "            s = segment.symbol\n",
    "        except AttributeError:\n",
    "            s = segment\n",
    "        if s in sp:\n",
    "            return 'NEUTR:'+''.join(str(x) for x in sp)\n",
    "    return s\n",
    "\n",
    "\n",
    "# I updated the doc strings on this one.\n",
    "# This one also now has two different \"relative count\" options.\n",
    "def all_pairwise_fls(corpus_context, relative_fl = False,\n",
    "                    algorithm = 'minpair',\n",
    "                    relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = True, distinguish_homophones = False,\n",
    "                    environment_filter = None):\n",
    "    \"\"\"Calculate the functional load of the contrast between two segments as a count of minimal pairs.\n",
    "    This version calculates the functional load for ALL pairs of segments in the inventory,\n",
    "    which could be useful for visually mapping out phoneme inventories.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus_context : CorpusContext\n",
    "        Context manager for a corpus\n",
    "    relative_fl : bool\n",
    "        If False, return the FL for all segment pairs. If True, return\n",
    "        the relative (average) FL for each segment.\n",
    "    algorithm : str {'minpair', 'deltah'}\n",
    "        Algorithm to use for calculating functional load: \"minpair\" for\n",
    "        minimal pair count or \"deltah\" for change in entropy.\n",
    "    relative_count_to_relevant_sounds : bool, optional\n",
    "        If True, divide the number of minimal pairs by the total count\n",
    "        by the total number of words that contain either of the two segments.\n",
    "    relative_count_to_whole_corpus : bool, optional\n",
    "        If True, divide the number of minimal pairs by the total number of words \n",
    "        in the corpus (regardless of whether those words contain the target sounds).\n",
    "        Defaults to True.\n",
    "    distinguish_homophones : bool, optional\n",
    "        If False, then you'll count sock~shock (sock=clothing) and\n",
    "        sock~shock (sock=punch) as just one minimal pair; but if True,\n",
    "        you'll overcount alternative spellings of the same word, e.g.\n",
    "        axel~actual and axle~actual. False is the value used by Wedel et al.\n",
    "    environment_filter : EnvironmentFilter\n",
    "        Allows the user to restrict the neutralization process to segments in\n",
    "        particular segmental contexts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of tuple(tuple(str, st), float)\n",
    "    OR\n",
    "    list of (str, float)\n",
    "        Normally returns a list of all Segment pairs and their respective functional load values, as length-2 tuples ordered by FL.\n",
    "        If calculating relative FL (i.e., average FL for a segment), returns a dictionary of each segment and its relative (average) FL, with entries ordered by FL.\n",
    "    \"\"\"\n",
    "    fls = {}\n",
    "    total_calculations = ((((len(corpus_context.inventory)-1)**2)-len(corpus_context.inventory)-1)/2)+1\n",
    "    ct = 1\n",
    "    t = time.time()\n",
    "    if '' in corpus_context.inventory:\n",
    "        raise Exception('Warning: Calculation of functional load for all segment pairs requires that all items in corpus have a non-null transcription.')\n",
    "    \n",
    "    # Count the number of words in the corpus (needed if relative_count_to_whole_corpus is True)\n",
    "    num_words_in_corpus = len(corpus_context.corpus)\n",
    "    \n",
    "    for i, s1 in enumerate(corpus_context.inventory[:-1]):\n",
    "        for s2 in corpus_context.inventory[i+1:]:\n",
    "            if s1 != '#' and s2 != '#':\n",
    "                print('Performing FL calculation {} out of {} possible'.format(str(ct), str(total_calculations)))\n",
    "                ct += 1\n",
    "                print('Duration of last calculation: {}'.format(str(time.time() - t)))\n",
    "                t = time.time()\n",
    "                if type(s1) != str:\n",
    "                    s1 = s1.symbol\n",
    "                if type(s2) != str:\n",
    "                    s2 = s2.symbol\n",
    "                if algorithm == 'minpair':\n",
    "                    fl = minpair_fl(corpus_context, [(s1, s2)],\n",
    "                            relative_count_to_relevant_sounds=relative_count_to_relevant_sounds,\n",
    "                            relative_count_to_whole_corpus=relative_count_to_whole_corpus,\n",
    "                            distinguish_homophones=distinguish_homophones,\n",
    "                            environment_filter=environment_filter)[0]\n",
    "                elif algorithm == 'deltah':\n",
    "                    fl = deltah_fl(corpus_context, [(s1, s2)],\n",
    "                    environment_filter=environment_filter)\n",
    "                fls[(s1, s2)] = fl\n",
    "    if not relative_fl:\n",
    "        ordered_fls = sorted([(pair, fls[pair]) for pair in fls], key=lambda p: p[1], reverse=True)\n",
    "        return ordered_fls\n",
    "    elif relative_fl:\n",
    "        rel_fls = {}\n",
    "        for s in corpus_context.inventory:\n",
    "            if type(s) != str:\n",
    "                s = s.symbol\n",
    "            if s != '#':\n",
    "                total = 0.0\n",
    "                for pair in fls:\n",
    "                    if s == pair[0] or s == pair[1]:\n",
    "                        total += fls[pair]\n",
    "                rel_fls[s] = total / (len(corpus_context.inventory) - 1)\n",
    "        ordered_rel_fls = sorted([(s, rel_fls[s]) for s in rel_fls], key=lambda p: p[1], reverse=True)\n",
    "        return ordered_rel_fls\n",
    "\n",
    "    \n",
    "    ## Filter out words that have none of the target segments\n",
    "    ## (for relative_count_to_relevant_sounds as well as improving runtime)\n",
    "    contain_target_segment = []\n",
    "    if call_back is not None:\n",
    "        call_back('Finding words with the specified segments...')\n",
    "        call_back(0, len(corpus_context))\n",
    "        cur = 0\n",
    "\n",
    "    all_target_segments = list(itertools.chain.from_iterable(segment_pairs)) # creates a list of target segments from the list of tuples\n",
    "    for w in corpus_context: # loops through the words in the context manager\n",
    "        if stop_check is not None and stop_check():\n",
    "            return\n",
    "        if call_back is not None:\n",
    "            cur += 1\n",
    "            if cur % 100 == 0:\n",
    "                call_back(cur)\n",
    "        tier = getattr(w, corpus_context.sequence_type)\n",
    "        if any([s in tier for s in all_target_segments]):\n",
    "                contain_target_segment.append(w)\n",
    "    if stop_check is not None and stop_check():\n",
    "        return\n",
    "\n",
    "    ## Find minimal pairs\n",
    "    minpairs = []\n",
    "    if call_back is not None:\n",
    "        call_back('Finding minimal pairs...')\n",
    "        if len(contain_target_segment) >= 2:\n",
    "            call_back(0,factorial(len(contain_target_segment))/(factorial(len(contain_target_segment)-2)*2))\n",
    "        cur = 0\n",
    "    for first, second in itertools.combinations(contain_target_segment, 2):\n",
    "        if stop_check is not None and stop_check():\n",
    "            return\n",
    "        if call_back is not None:\n",
    "            cur += 1\n",
    "            if cur % 100 == 0:\n",
    "                call_back(cur)\n",
    "        if is_minpair(first, second, corpus_context, segment_pairs, environment_filter):\n",
    "            ordered_pair = sorted([(first, getattr(first, corpus_context.sequence_type)),\n",
    "                                   (second, getattr(second, corpus_context.sequence_type))],\n",
    "                                   key = lambda x: x[1]) # sort by tier/transcription\n",
    "            minpairs.append(tuple(ordered_pair))\n",
    "\n",
    "    ## Generate output \n",
    "    if not distinguish_homophones:\n",
    "        actual_minpairs = {}\n",
    "\n",
    "        for pair in minpairs:\n",
    "            if stop_check is not None and stop_check():\n",
    "                return\n",
    "            key = (pair[0][1], pair[1][1]) # Keys are tuples of transcriptions\n",
    "            if key not in actual_minpairs:\n",
    "                actual_minpairs[key] = (pair[0][0], pair[1][0]) # Values are words\n",
    "            else:\n",
    "                pair_freq = pair[0][0].frequency + pair[1][0].frequency\n",
    "                existing_freq = actual_minpairs[key][0].frequency + \\\n",
    "                                actual_minpairs[key][1].frequency\n",
    "                if pair_freq > existing_freq:\n",
    "                    actual_minpairs[key] = (pair[0][0], pair[1][0])\n",
    "        result = sum((x[0].frequency + x[1].frequency)/2\n",
    "                    for x in actual_minpairs.values())\n",
    "    else:\n",
    "        result = sum((x[0][0].frequency + x[1][0].frequency)/2 for x in minpairs)\n",
    "\n",
    "    if relative_count_to_relevant_sounds and len(contain_target_segment) > 0:\n",
    "        result /= sum(x.frequency for x in contain_target_segment)\n",
    "\n",
    "    # added what to do if the relative count to the whole corpus is true, namely, divide by the number of words in the corpus    \n",
    "    elif relative_count_to_whole_corpus:\n",
    "        result = result / num_words_in_corpus\n",
    "    \n",
    "    return (result, minpairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for testing purposes, define the corpus context classes\n",
    "\n",
    "class BaseCorpusContext(object):\n",
    "    \"\"\"\n",
    "    Abstract Corpus context class that all other contexts inherit from.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : Corpus\n",
    "        Corpus to form context from\n",
    "    sequence_type : str\n",
    "        Sequence type to evaluate algorithms on (i.e., 'transcription')\n",
    "    type_or_token : str\n",
    "        The type of frequency to use for calculations\n",
    "    attribute : Attribute, optional\n",
    "        Attribute to save results to for calculations involving all words\n",
    "        in the Corpus\n",
    "    frequency_threshold: float, optional\n",
    "        If specified, ignore words below this token frequency\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus, sequence_type, type_or_token,\n",
    "                attribute = None, frequency_threshold = 0):\n",
    "        self.sequence_type = sequence_type\n",
    "        self.type_or_token = type_or_token\n",
    "        self.corpus = corpus\n",
    "        self.attribute = attribute\n",
    "        self._freq_base = {}\n",
    "        self.length = None\n",
    "        self.frequency_threshold = frequency_threshold\n",
    "\n",
    "    @property\n",
    "    def inventory(self):\n",
    "        return self.corpus.inventory\n",
    "\n",
    "    @property\n",
    "    def specifier(self):\n",
    "        return self.corpus.specifier\n",
    "\n",
    "    def __enter__(self):\n",
    "        if self.attribute is not None:\n",
    "            self.corpus.add_attribute(self.attribute,initialize_defaults = False)\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.length is not None:\n",
    "            return self.length\n",
    "        else:\n",
    "            counter = 0\n",
    "            for w in self:\n",
    "                counter += 1\n",
    "            self.length = counter\n",
    "            return self.length\n",
    "\n",
    "    def get_frequency_base(self, gramsize = 1, halve_edges = False, probability = False):\n",
    "        \"\"\"\n",
    "        Generate (and cache) frequencies for each segment in the Corpus.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        halve_edges : boolean\n",
    "            If True, word boundary symbols ('#') will only be counted once\n",
    "            per word, rather than twice.  Defaults to False.\n",
    "\n",
    "        gramsize : integer\n",
    "            Size of n-gram to use for getting frequency, defaults to 1 (unigram)\n",
    "\n",
    "        probability : boolean\n",
    "            If True, frequency counts will be normalized by total frequency,\n",
    "            defaults to False\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Keys are segments (or sequences of segments) and values are\n",
    "            their frequency in the Corpus\n",
    "        \"\"\"\n",
    "        if (gramsize) not in self._freq_base:\n",
    "            freq_base = collections.defaultdict(float)\n",
    "            for word in self:\n",
    "                tier = getattr(word, self.sequence_type)\n",
    "                if self.sequence_type == 'spelling':\n",
    "                    seq = ['#'] + [x for x in tier] + ['#']\n",
    "                else:\n",
    "                    seq = tier.with_word_boundaries()\n",
    "                grams = zip(*[seq[i:] for i in range(gramsize)])\n",
    "                for x in grams:\n",
    "                    if len(x) == 1:\n",
    "                        x = x[0]\n",
    "                    freq_base[x] += word.frequency\n",
    "            freq_base['total'] = sum(value for value in freq_base.values())\n",
    "            self._freq_base[(gramsize)] = freq_base\n",
    "        freq_base = self._freq_base[(gramsize)]\n",
    "        return_dict = { k:v for k,v in freq_base.items()}\n",
    "        if halve_edges and '#' in return_dict:\n",
    "            return_dict['#'] = (return_dict['#'] / 2) + 1\n",
    "            if not probability:\n",
    "                return_dict['total'] -= return_dict['#'] - 2\n",
    "        if probability:\n",
    "            return_dict = { k:v/freq_base['total'] for k,v in return_dict.items()}\n",
    "        return return_dict\n",
    "\n",
    "    def get_phone_probs(self, gramsize = 1, probability = True, preserve_position = True, log_count = True):\n",
    "        \"\"\"\n",
    "        Generate (and cache) phonotactic probabilities for segments in\n",
    "        the Corpus.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        gramsize : integer\n",
    "            Size of n-gram to use for getting frequency, defaults to 1 (unigram)\n",
    "\n",
    "        probability : boolean\n",
    "            If True, frequency counts will be normalized by total frequency,\n",
    "            defaults to False\n",
    "\n",
    "        preserve_position : boolean\n",
    "            If True, segments will in different positions in the transcription\n",
    "            will not be collapsed, defaults to True\n",
    "\n",
    "        log_count : boolean\n",
    "            If True, token frequencies will be logrithmically-transformed\n",
    "            prior to being summed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Keys are segments (or sequences of segments) and values are\n",
    "            their phonotactic probability in the Corpus\n",
    "        \"\"\"\n",
    "        if (gramsize, preserve_position, log_count) not in self._freq_base:\n",
    "            freq_base = collections.defaultdict(float)\n",
    "            totals = collections.defaultdict(float)\n",
    "            for word in self:\n",
    "                freq = word.frequency\n",
    "                if self.type_or_token != 'type' and log_count:\n",
    "                    freq = math.log(freq)\n",
    "                grams = zip(*[getattr(word, self.sequence_type)[i:] for i in range(gramsize)])\n",
    "\n",
    "                for i, x in enumerate(grams):\n",
    "                    #if len(x) == 1:\n",
    "                    #    x = x[0]\n",
    "                    if preserve_position:\n",
    "                        x = (x,i)\n",
    "                        totals[i] += freq\n",
    "                    freq_base[x] += freq\n",
    "\n",
    "            if not preserve_position:\n",
    "                freq_base['total'] = sum(value for value in freq_base.values())\n",
    "            else:\n",
    "                freq_base['total'] = totals\n",
    "            self._freq_base[(gramsize, preserve_position, log_count)] = freq_base\n",
    "\n",
    "        freq_base = self._freq_base[(gramsize,preserve_position, log_count)]\n",
    "        return_dict = { k:v for k,v in freq_base.items()}\n",
    "        if probability and not preserve_position:\n",
    "            return_dict = { k:v/freq_base['total'] for k,v in return_dict.items()}\n",
    "        elif probability:\n",
    "            return_dict = { k:v/freq_base['total'][k[1]] for k,v in return_dict.items() if k != 'total'}\n",
    "        return return_dict\n",
    "\n",
    "    def __exit__(self, exc_type, exc, exc_tb):\n",
    "        if exc_type is None:\n",
    "            return True\n",
    "        else:\n",
    "            if self.attribute is not None:\n",
    "                self.corpus.remove_attribute(self.attribute)\n",
    "\n",
    "\n",
    "class CanonicalVariantContext(BaseCorpusContext):\n",
    "    \"\"\"\n",
    "    Corpus context that uses canonical forms for transcriptions and tiers\n",
    "\n",
    "    See the documentation of `BaseCorpusContext` for additional information\n",
    "    \"\"\"\n",
    "    def __exit__(self, exc_type, exc, exc_tb):\n",
    "        BaseCorpusContext.__exit__(self, exc_type, exc, exc_tb)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for word in self.corpus:\n",
    "            if math.isnan(word.frequency):\n",
    "                continue\n",
    "            if self.type_or_token == 'token' and word.frequency == 0:\n",
    "                continue\n",
    "            if self.frequency_threshold > 0 and word.frequency < self.frequency_threshold:\n",
    "                continue\n",
    "            w = copy.copy(word)\n",
    "            if self.type_or_token == 'type':\n",
    "                w.frequency = 1\n",
    "            w.original = word\n",
    "            yield w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "# extra packages to allow import of corpus\n",
    "from corpustools.corpus.io import binary\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/KCH/Desktop/CorpusTools_KCH_fork/CorpusTools/lemurian.corpus\n"
     ]
    }
   ],
   "source": [
    "# create path and load in corpus\n",
    "path = os.path.join(os.getcwd(), 'lemurian.corpus')\n",
    "print(path)\n",
    "corpus = binary.load_binary(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a corpus context\n",
    "corpus_context = CanonicalVariantContext(corpus, \"transcription\", \"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in the corpus is: \n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# get the length of the corpus so we know what the relative counts are relative to\n",
    "num_words_in_corpus = len(corpus_context.corpus)\n",
    "\n",
    "print(\"The number of words in the corpus is: \")\n",
    "print(num_words_in_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in the corpus with the target segments is: \n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# get the number of words in the corpus that contain either [s] or [l]:\n",
    "all_target_segments = [\"s\",\"l\"]\n",
    "words_with_targets = []\n",
    "for w in corpus_context:\n",
    " tier = getattr(w, corpus_context.sequence_type)\n",
    " if any([s in tier for s in all_target_segments]):\n",
    "     words_with_targets.append(w)\n",
    "\n",
    "num_words_with_targets = len(words_with_targets)\n",
    "\n",
    "print(\"The number of words in the corpus with the target segments is: \")\n",
    "print(num_words_with_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw number of minimal pairs based on [s]/[l] in the corpus is: \n",
      "0.05\n",
      "And the minimal pairs are: \n",
      "Pair 1 is:  l.u.n.o.b.a / s.u.n.o.b.a\n",
      "Pair 2 is:  l.w.a.f.w.a.p.u / s.w.a.f.w.a.p.u\n",
      "[((<Word: 'luNoba'>, l.u.n.o.b.a), (<Word: 'suNoba'>, s.u.n.o.b.a)), ((<Word: 'lwafwapu'>, l.w.a.f.w.a.p.u), (<Word: 'swafwapu'>, s.w.a.f.w.a.p.u))]\n"
     ]
    }
   ],
   "source": [
    "# Get the raw count of minimal pairs hinging on s / l\n",
    "raw_s_l = minpair_fl(corpus_context, [('s','l')],\n",
    "        relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = True, distinguish_homophones = False,\n",
    "        environment_filter = None,\n",
    "        stop_check = None, call_back = None)\n",
    "\n",
    "print(\"The raw number of minimal pairs based on [s]/[l] in the corpus is: \")\n",
    "print(raw_s_l[0])\n",
    "print(\"And the minimal pairs are: \")\n",
    "\n",
    "for mp in range(len(raw_s_l[1])):\n",
    "    print(\"Pair\", mp+1, \"is: \", raw_s_l[1][mp][0][1], \"/\", raw_s_l[1][mp][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(raw_s_l[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minpair_fl(corpus_context, [('s','l')],\n",
    "        relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = True, distinguish_homophones = False,\n",
    "        environment_filter = None,\n",
    "        stop_check = None, call_back = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CanonicalVariantContext"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meseta\n",
      "oN\n"
     ]
    }
   ],
   "source": [
    "word1 = corpus.random_word()\n",
    "word2 = corpus.random_word()\n",
    "print(word1)\n",
    "print(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_minpair(word1, word2, corpus_context, [('s','l')], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for w in corpus:\n",
    "    if str(w) == 'luNoba':\n",
    "        word1 = w\n",
    "    elif str(w) == 'suNoba':\n",
    "        word2 = w\n",
    "    else:\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "luNoba\n",
      "suNoba\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(word1)\n",
    "print(word2)\n",
    "is_minpair(word1, word2, corpus_context, [('s','l')], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minpair_fl(corpus_context, [('s','l')],\n",
    "        relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = False, distinguish_homophones = False,\n",
    "        environment_filter = None,\n",
    "        stop_check = None, call_back = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minpair_fl(corpus_context, segment_pairs,\n",
    "        relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = True, distinguish_homophones = False,\n",
    "        environment_filter = None,\n",
    "        stop_check = None, call_back = None):\n",
    "    \"\"\"Calculate the functional load of the contrast between two segments\n",
    "    as a count of minimal pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus_context : CorpusContext\n",
    "        Context manager for a corpus\n",
    "    segment_pairs : list of length-2 tuples of str\n",
    "        The pairs of segments to be conflated.\n",
    "    relative_count_to_relevant_sounds : bool, optional\n",
    "        If True, divide the number of minimal pairs by \n",
    "        by the total number of words that contain either of the two segments.\n",
    "        # changed the name of this from \"relative_count\" to \"relative_count_to_relevant_sounds\"\n",
    "        # set its default to False above\n",
    "    relative_count_to_whole_corpus : bool, optional\n",
    "        If True, divide the number of minimal pairs by the total number of words \n",
    "        in the corpus (regardless of whether those words contain the target sounds).\n",
    "        Defaults to True.\n",
    "    distinguish_homophones : bool, optional\n",
    "        If False, then you'll count sock~shock (sock=clothing) and\n",
    "        sock~shock (sock=punch) as just one minimal pair; but if True,\n",
    "        you'll overcount alternative spellings of the same word, e.g.\n",
    "        axel~actual and axle~actual. False is the value used by Wedel et al.\n",
    "    environment_filter : EnvironmentFilter\n",
    "        Allows the user to restrict the neutralization process to segments in\n",
    "        particular segmental contexts\n",
    "    stop_check : callable, optional\n",
    "        Optional function to check whether to gracefully terminate early\n",
    "    call_back : callable, optional\n",
    "        Optional function to supply progress information during the function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple(int or float, list)\n",
    "        Tuple of: 0. if `relative_count_to_relevant_sounds`==False and \n",
    "        `relative_count_to_whole_corpus`==False, an int of the raw number of\n",
    "        minimal pairs; if `relative_count_to_relevant_sounds`==True, a float of that\n",
    "        count divided by the total number of words in the corpus that\n",
    "        include either `s1` or `s2`; if `relative_count_to_whole_corpus`==True, a\n",
    "        float of the raw count divided by the total number of words in the corpus; \n",
    "        and 1. list of minimal pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    if stop_check is not None and stop_check():\n",
    "        return\n",
    "\n",
    "    ## Count the number of words in the corpus (needed if relative_count_to_whole_corpus is True)\n",
    "    num_words_in_corpus = len(corpus_context.corpus) \n",
    "    \n",
    "    ## Filter out words that have none of the target segments\n",
    "    ## (for relative_count_to_relevant_sounds as well as improving runtime)\n",
    "    contain_target_segment = []\n",
    "    if call_back is not None:\n",
    "        call_back('Finding words with the specified segments...')\n",
    "        call_back(0, len(corpus_context))\n",
    "        cur = 0\n",
    "\n",
    "    all_target_segments = list(itertools.chain.from_iterable(segment_pairs)) # creates a list of target segments from the list of tuples\n",
    "    for w in corpus_context: # loops through the words in the context manager?\n",
    "        if stop_check is not None and stop_check():\n",
    "            return\n",
    "        if call_back is not None:\n",
    "            cur += 1\n",
    "            if cur % 100 == 0:\n",
    "                call_back(cur)\n",
    "        tier = getattr(w, corpus_context.sequence_type)\n",
    "        if any([s in tier for s in all_target_segments]):\n",
    "                contain_target_segment.append(w)\n",
    "    if stop_check is not None and stop_check():\n",
    "        return\n",
    "\n",
    "    ## Find minimal pairs\n",
    "    minpairs = []\n",
    "    if call_back is not None:\n",
    "        call_back('Finding minimal pairs...')\n",
    "        if len(contain_target_segment) >= 2:\n",
    "            call_back(0,factorial(len(contain_target_segment))/(factorial(len(contain_target_segment)-2)*2))\n",
    "        cur = 0\n",
    "    for first, second in itertools.combinations(contain_target_segment, 2):\n",
    "        if stop_check is not None and stop_check():\n",
    "            return\n",
    "        if call_back is not None:\n",
    "            cur += 1\n",
    "            if cur % 100 == 0:\n",
    "                call_back(cur)\n",
    "        if is_minpair(first, second, corpus_context, segment_pairs, environment_filter):\n",
    "            ordered_pair = sorted([(first, getattr(first, corpus_context.sequence_type)),\n",
    "                                   (second, getattr(second, corpus_context.sequence_type))],\n",
    "                                   key = lambda x: x[1]) # sort by tier/transcription\n",
    "            minpairs.append(tuple(ordered_pair))\n",
    "\n",
    "    ## Generate output \n",
    "    if not distinguish_homophones:\n",
    "        actual_minpairs = {}\n",
    "\n",
    "        for pair in minpairs:\n",
    "            if stop_check is not None and stop_check():\n",
    "                return\n",
    "            key = (pair[0][1], pair[1][1]) # Keys are tuples of transcriptions\n",
    "            if key not in actual_minpairs:\n",
    "                actual_minpairs[key] = (pair[0][0], pair[1][0]) # Values are words\n",
    "            else:\n",
    "                pair_freq = pair[0][0].frequency + pair[1][0].frequency\n",
    "                existing_freq = actual_minpairs[key][0].frequency + \\\n",
    "                                actual_minpairs[key][1].frequency\n",
    "                if pair_freq > existing_freq:\n",
    "                    actual_minpairs[key] = (pair[0][0], pair[1][0])\n",
    "        result = sum((x[0].frequency + x[1].frequency)/2\n",
    "                    for x in actual_minpairs.values())\n",
    "    else:\n",
    "        result = sum((x[0][0].frequency + x[1][0].frequency)/2 for x in minpairs)\n",
    "\n",
    "    if relative_count_to_relevant_sounds and len(contain_target_segment) > 0:\n",
    "        result /= sum(x.frequency for x in contain_target_segment)\n",
    "        \n",
    "        # Q: do I need to put anything here between these if statements (like make it an else if)? or can I just have them back to back?\n",
    "        # relative_count_to_relevant_sounds and relative_count_to_whole_corpus won't both be true\n",
    "        # but both COULD be false, in which case, result should just stay as the raw count \n",
    "        # that I assume is returned above -- elif will ensure that only one runs; if will allow both to run\n",
    "        \n",
    "    elif relative_count_to_whole_corpus:\n",
    "        result = result / num_words_in_corpus\n",
    "    \n",
    "    return (result, minpairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0,\n",
       " [((<Word: 'luNoba'>, l.u.n.o.b.a), (<Word: 'suNoba'>, s.u.n.o.b.a)),\n",
       "  ((<Word: 'lwafwapu'>, l.w.a.f.w.a.p.u),\n",
       "   (<Word: 'swafwapu'>, s.w.a.f.w.a.p.u))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minpair_fl(corpus_context, [('s','l')],\n",
    "        relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = False, distinguish_homophones = False,\n",
    "        environment_filter = None,\n",
    "        stop_check = None, call_back = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the average functional load for the segment [s]\n",
    "avg_s_fl = relative_minpair_fl(corpus_context, 's',\n",
    "            relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = True, \n",
    "            distinguish_homophones = False, output_filename = None, environment_filter = None,\n",
    "            stop_check = None, call_back = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0075\n"
     ]
    }
   ],
   "source": [
    "print(avg_s_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<corpustools.corpus.classes.lexicon.Inventory object at 0x105d665f8>\n"
     ]
    }
   ],
   "source": [
    "print(corpus_context.corpus.inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "d\n",
      "e\n",
      "f\n",
      "i\n",
      "j\n",
      "k\n",
      "l\n",
      "m\n",
      "n\n",
      "o\n",
      "p\n",
      "r\n",
      "s\n",
      "t\n",
      "u\n",
      "w\n",
      "x\n",
      "y\n",
      "z\n"
     ]
    }
   ],
   "source": [
    "for s in corpus_context.corpus.inventory:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a firm list of all the pairs of segments that contain [s], other than itslf.\n",
    "segment_pairs_with_s = []\n",
    "for s in corpus_context.corpus.inventory:\n",
    "    if str(s) == 's':\n",
    "        next\n",
    "    else:\n",
    "        this_pair = [('s', str(s))]    \n",
    "        segment_pairs_with_s.append(this_pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('s', 'a')], [('s', 'b')], [('s', 'd')], [('s', 'e')], [('s', 'f')], [('s', 'i')], [('s', 'j')], [('s', 'k')], [('s', 'l')], [('s', 'm')], [('s', 'n')], [('s', 'o')], [('s', 'p')], [('s', 'r')], [('s', 't')], [('s', 'u')], [('s', 'w')], [('s', 'x')], [('s', 'y')], [('s', 'z')]]\n"
     ]
    }
   ],
   "source": [
    "print(segment_pairs_with_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the functional load of each pair.\n",
    "raw_fls_for_s_pairs = []\n",
    "rel_fls_for_s_pairs_to_corpus = []\n",
    "for pair in segment_pairs_with_s:\n",
    "    this_raw_fl = minpair_fl(corpus_context, pair, relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = False, distinguish_homophones = False, environment_filter = None, stop_check = None, call_back = None)\n",
    "    this_rel_fl = minpair_fl(corpus_context, pair, relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = True, distinguish_homophones = False, environment_filter = None, stop_check = None, call_back = None)\n",
    "    raw_fls_for_s_pairs.append(this_raw_fl[0])\n",
    "    rel_fls_for_s_pairs_to_corpus.append(this_rel_fl[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 2.0, 0, 1.0, 0, 0, 2.0, 1.0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_fls_for_s_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05,\n",
       " 0.0,\n",
       " 0.025,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.05,\n",
       " 0.025,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_fls_for_s_pairs_to_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# got help with using Pandas to display the data from: http://pbpython.com/pandas-list-dict.html\n",
    "# create a dictionary:\n",
    "fls = {'Pairs': segment_pairs_with_s, 'Raw FL counts': raw_fls_for_s_pairs, 'FLs Rel to Corpus': rel_fls_for_s_pairs_to_corpus}\n",
    "\n",
    "# create a dataframe:\n",
    "df = pandas.DataFrame.from_dict(fls)\n",
    "\n",
    "# order the columns:\n",
    "df = df[['Pairs', 'Raw FL counts', 'FLs Rel to Corpus']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are all the pairs with [s], and their FLs relative to the corpus size:\n",
      "       Pairs  Raw FL counts  FLs Rel to Corpus\n",
      "0   [(s, a)]            0.0              0.000\n",
      "1   [(s, b)]            0.0              0.000\n",
      "2   [(s, d)]            0.0              0.000\n",
      "3   [(s, e)]            0.0              0.000\n",
      "4   [(s, f)]            0.0              0.000\n",
      "5   [(s, i)]            0.0              0.000\n",
      "6   [(s, j)]            0.0              0.000\n",
      "7   [(s, k)]            0.0              0.000\n",
      "8   [(s, l)]            2.0              0.050\n",
      "9   [(s, m)]            0.0              0.000\n",
      "10  [(s, n)]            1.0              0.025\n",
      "11  [(s, o)]            0.0              0.000\n",
      "12  [(s, p)]            0.0              0.000\n",
      "13  [(s, r)]            2.0              0.050\n",
      "14  [(s, t)]            1.0              0.025\n",
      "15  [(s, u)]            0.0              0.000\n",
      "16  [(s, w)]            0.0              0.000\n",
      "17  [(s, x)]            0.0              0.000\n",
      "18  [(s, y)]            0.0              0.000\n",
      "19  [(s, z)]            0.0              0.000\n"
     ]
    }
   ],
   "source": [
    "print(\"Here are all the pairs with [s], and their FLs relative to the corpus size:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaging the FLs relative to the corpus gives us: \n",
      "0.0075\n"
     ]
    }
   ],
   "source": [
    "print(\"Averaging the FLs relative to the corpus gives us: \")\n",
    "print(sum(rel_fls_for_s_pairs_to_corpus) / len(rel_fls_for_s_pairs_to_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the average functional load for the segment [s]\n",
    "avg_s_fl = relative_minpair_fl(corpus_context, 's',\n",
    "            relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = True, \n",
    "            distinguish_homophones = False, output_filename = None, environment_filter = None,\n",
    "            stop_check = None, call_back = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the built-in function to calculate the average functional load of [s], relative to the corpus, gives us: \n",
      "0.0075\n"
     ]
    }
   ],
   "source": [
    "print(\"Using the built-in function to calculate the average functional load of [s], relative to the corpus, gives us: \")\n",
    "print(avg_s_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing FL calculation 1 out of 210.0 possible\n",
      "Duration of last calculation: 0.0004200935363769531\n",
      "Performing FL calculation 2 out of 210.0 possible\n",
      "Duration of last calculation: 0.0065958499908447266\n",
      "Performing FL calculation 3 out of 210.0 possible\n",
      "Duration of last calculation: 0.0042591094970703125\n",
      "Performing FL calculation 4 out of 210.0 possible\n",
      "Duration of last calculation: 0.008010149002075195\n",
      "Performing FL calculation 5 out of 210.0 possible\n",
      "Duration of last calculation: 0.007782936096191406\n",
      "Performing FL calculation 6 out of 210.0 possible\n",
      "Duration of last calculation: 0.004227876663208008\n",
      "Performing FL calculation 7 out of 210.0 possible\n",
      "Duration of last calculation: 0.0038580894470214844\n",
      "Performing FL calculation 8 out of 210.0 possible\n",
      "Duration of last calculation: 0.005745887756347656\n",
      "Performing FL calculation 9 out of 210.0 possible\n",
      "Duration of last calculation: 0.0066890716552734375\n",
      "Performing FL calculation 10 out of 210.0 possible\n",
      "Duration of last calculation: 0.006606101989746094\n",
      "Performing FL calculation 11 out of 210.0 possible\n",
      "Duration of last calculation: 0.005202054977416992\n",
      "Performing FL calculation 12 out of 210.0 possible\n",
      "Duration of last calculation: 0.006749153137207031\n",
      "Performing FL calculation 13 out of 210.0 possible\n",
      "Duration of last calculation: 0.005321025848388672\n",
      "Performing FL calculation 14 out of 210.0 possible\n",
      "Duration of last calculation: 0.0038301944732666016\n",
      "Performing FL calculation 15 out of 210.0 possible\n",
      "Duration of last calculation: 0.006426095962524414\n",
      "Performing FL calculation 16 out of 210.0 possible\n",
      "Duration of last calculation: 0.006114006042480469\n",
      "Performing FL calculation 17 out of 210.0 possible\n",
      "Duration of last calculation: 0.0339970588684082\n",
      "Performing FL calculation 18 out of 210.0 possible\n",
      "Duration of last calculation: 0.007303953170776367\n",
      "Performing FL calculation 19 out of 210.0 possible\n",
      "Duration of last calculation: 0.006567955017089844\n",
      "Performing FL calculation 20 out of 210.0 possible\n",
      "Duration of last calculation: 0.006525993347167969\n",
      "Performing FL calculation 21 out of 210.0 possible\n",
      "Duration of last calculation: 0.007450103759765625\n",
      "Performing FL calculation 22 out of 210.0 possible\n",
      "Duration of last calculation: 0.005059003829956055\n",
      "Performing FL calculation 23 out of 210.0 possible\n",
      "Duration of last calculation: 0.006011962890625\n",
      "Performing FL calculation 24 out of 210.0 possible\n",
      "Duration of last calculation: 0.0030629634857177734\n",
      "Performing FL calculation 25 out of 210.0 possible\n",
      "Duration of last calculation: 0.0031328201293945312\n",
      "Performing FL calculation 26 out of 210.0 possible\n",
      "Duration of last calculation: 0.0029158592224121094\n",
      "Performing FL calculation 27 out of 210.0 possible\n",
      "Duration of last calculation: 0.0030438899993896484\n",
      "Performing FL calculation 28 out of 210.0 possible\n",
      "Duration of last calculation: 0.0055539608001708984\n",
      "Performing FL calculation 29 out of 210.0 possible\n",
      "Duration of last calculation: 0.003197908401489258\n",
      "Performing FL calculation 30 out of 210.0 possible\n",
      "Duration of last calculation: 0.0032019615173339844\n",
      "Performing FL calculation 31 out of 210.0 possible\n",
      "Duration of last calculation: 0.003416776657104492\n",
      "Performing FL calculation 32 out of 210.0 possible\n",
      "Duration of last calculation: 0.005107879638671875\n",
      "Performing FL calculation 33 out of 210.0 possible\n",
      "Duration of last calculation: 0.005231142044067383\n",
      "Performing FL calculation 34 out of 210.0 possible\n",
      "Duration of last calculation: 0.005782127380371094\n",
      "Performing FL calculation 35 out of 210.0 possible\n",
      "Duration of last calculation: 0.004168987274169922\n",
      "Performing FL calculation 36 out of 210.0 possible\n",
      "Duration of last calculation: 0.003468036651611328\n",
      "Performing FL calculation 37 out of 210.0 possible\n",
      "Duration of last calculation: 0.005451202392578125\n",
      "Performing FL calculation 38 out of 210.0 possible\n",
      "Duration of last calculation: 0.005329132080078125\n",
      "Performing FL calculation 39 out of 210.0 possible\n",
      "Duration of last calculation: 0.0028460025787353516\n",
      "Performing FL calculation 40 out of 210.0 possible\n",
      "Duration of last calculation: 0.003593921661376953\n",
      "Performing FL calculation 41 out of 210.0 possible\n",
      "Duration of last calculation: 0.003493070602416992\n",
      "Performing FL calculation 42 out of 210.0 possible\n",
      "Duration of last calculation: 0.003340005874633789\n",
      "Performing FL calculation 43 out of 210.0 possible\n",
      "Duration of last calculation: 0.005780935287475586\n",
      "Performing FL calculation 44 out of 210.0 possible\n",
      "Duration of last calculation: 0.005267143249511719\n",
      "Performing FL calculation 45 out of 210.0 possible\n",
      "Duration of last calculation: 0.00522303581237793\n",
      "Performing FL calculation 46 out of 210.0 possible\n",
      "Duration of last calculation: 0.003177165985107422\n",
      "Performing FL calculation 47 out of 210.0 possible\n",
      "Duration of last calculation: 0.0060269832611083984\n",
      "Performing FL calculation 48 out of 210.0 possible\n",
      "Duration of last calculation: 0.005491971969604492\n",
      "Performing FL calculation 49 out of 210.0 possible\n",
      "Duration of last calculation: 0.005426168441772461\n",
      "Performing FL calculation 50 out of 210.0 possible\n",
      "Duration of last calculation: 0.005446910858154297\n",
      "Performing FL calculation 51 out of 210.0 possible\n",
      "Duration of last calculation: 0.005151033401489258\n",
      "Performing FL calculation 52 out of 210.0 possible\n",
      "Duration of last calculation: 0.005205869674682617\n",
      "Performing FL calculation 53 out of 210.0 possible\n",
      "Duration of last calculation: 0.005049943923950195\n",
      "Performing FL calculation 54 out of 210.0 possible\n",
      "Duration of last calculation: 0.0057070255279541016\n",
      "Performing FL calculation 55 out of 210.0 possible\n",
      "Duration of last calculation: 0.003100156784057617\n",
      "Performing FL calculation 56 out of 210.0 possible\n",
      "Duration of last calculation: 0.005418062210083008\n",
      "Performing FL calculation 57 out of 210.0 possible\n",
      "Duration of last calculation: 0.005427837371826172\n",
      "Performing FL calculation 58 out of 210.0 possible\n",
      "Duration of last calculation: 0.0035500526428222656\n",
      "Performing FL calculation 59 out of 210.0 possible\n",
      "Duration of last calculation: 0.005059003829956055\n",
      "Performing FL calculation 60 out of 210.0 possible\n",
      "Duration of last calculation: 0.003859996795654297\n",
      "Performing FL calculation 61 out of 210.0 possible\n",
      "Duration of last calculation: 0.0063779354095458984\n",
      "Performing FL calculation 62 out of 210.0 possible\n",
      "Duration of last calculation: 0.006271839141845703\n",
      "Performing FL calculation 63 out of 210.0 possible\n",
      "Duration of last calculation: 0.00385284423828125\n",
      "Performing FL calculation 64 out of 210.0 possible\n",
      "Duration of last calculation: 0.006695985794067383\n",
      "Performing FL calculation 65 out of 210.0 possible\n",
      "Duration of last calculation: 0.006957054138183594\n",
      "Performing FL calculation 66 out of 210.0 possible\n",
      "Duration of last calculation: 0.007819175720214844\n",
      "Performing FL calculation 67 out of 210.0 possible\n",
      "Duration of last calculation: 0.004613161087036133\n",
      "Performing FL calculation 68 out of 210.0 possible\n",
      "Duration of last calculation: 0.006824970245361328\n",
      "Performing FL calculation 69 out of 210.0 possible\n",
      "Duration of last calculation: 0.006167888641357422\n",
      "Performing FL calculation 70 out of 210.0 possible\n",
      "Duration of last calculation: 0.005079030990600586\n",
      "Performing FL calculation 71 out of 210.0 possible\n",
      "Duration of last calculation: 0.005821943283081055\n",
      "Performing FL calculation 72 out of 210.0 possible\n",
      "Duration of last calculation: 0.0066318511962890625\n",
      "Performing FL calculation 73 out of 210.0 possible\n",
      "Duration of last calculation: 0.006443977355957031\n",
      "Performing FL calculation 74 out of 210.0 possible\n",
      "Duration of last calculation: 0.0036449432373046875\n",
      "Performing FL calculation 75 out of 210.0 possible\n",
      "Duration of last calculation: 0.004541873931884766\n",
      "Performing FL calculation 76 out of 210.0 possible\n",
      "Duration of last calculation: 0.0048961639404296875\n",
      "Performing FL calculation 77 out of 210.0 possible\n",
      "Duration of last calculation: 0.003000020980834961\n",
      "Performing FL calculation 78 out of 210.0 possible\n",
      "Duration of last calculation: 0.002974987030029297\n",
      "Performing FL calculation 79 out of 210.0 possible\n",
      "Duration of last calculation: 0.0031549930572509766\n",
      "Performing FL calculation 80 out of 210.0 possible\n",
      "Duration of last calculation: 0.0032880306243896484\n",
      "Performing FL calculation 81 out of 210.0 possible\n",
      "Duration of last calculation: 0.00302886962890625\n",
      "Performing FL calculation 82 out of 210.0 possible\n",
      "Duration of last calculation: 0.004112958908081055\n",
      "Performing FL calculation 83 out of 210.0 possible\n",
      "Duration of last calculation: 0.0030059814453125\n",
      "Performing FL calculation 84 out of 210.0 possible\n",
      "Duration of last calculation: 0.004426002502441406\n",
      "Performing FL calculation 85 out of 210.0 possible\n",
      "Duration of last calculation: 0.0030961036682128906\n",
      "Performing FL calculation 86 out of 210.0 possible\n",
      "Duration of last calculation: 0.0029439926147460938\n",
      "Performing FL calculation 87 out of 210.0 possible\n",
      "Duration of last calculation: 0.003939151763916016\n",
      "Performing FL calculation 88 out of 210.0 possible\n",
      "Duration of last calculation: 0.003275156021118164\n",
      "Performing FL calculation 89 out of 210.0 possible\n",
      "Duration of last calculation: 0.002958059310913086\n",
      "Performing FL calculation 90 out of 210.0 possible\n",
      "Duration of last calculation: 0.0031890869140625\n",
      "Performing FL calculation 91 out of 210.0 possible\n",
      "Duration of last calculation: 0.004063129425048828\n",
      "Performing FL calculation 92 out of 210.0 possible\n",
      "Duration of last calculation: 0.003386974334716797\n",
      "Performing FL calculation 93 out of 210.0 possible\n",
      "Duration of last calculation: 0.002981901168823242\n",
      "Performing FL calculation 94 out of 210.0 possible\n",
      "Duration of last calculation: 0.0031690597534179688\n",
      "Performing FL calculation 95 out of 210.0 possible\n",
      "Duration of last calculation: 0.004637002944946289\n",
      "Performing FL calculation 96 out of 210.0 possible\n",
      "Duration of last calculation: 0.0032868385314941406\n",
      "Performing FL calculation 97 out of 210.0 possible\n",
      "Duration of last calculation: 0.003899097442626953\n",
      "Performing FL calculation 98 out of 210.0 possible\n",
      "Duration of last calculation: 0.002986907958984375\n",
      "Performing FL calculation 99 out of 210.0 possible\n",
      "Duration of last calculation: 0.0032460689544677734\n",
      "Performing FL calculation 100 out of 210.0 possible\n",
      "Duration of last calculation: 0.0029909610748291016\n",
      "Performing FL calculation 101 out of 210.0 possible\n",
      "Duration of last calculation: 0.0036301612854003906\n",
      "Performing FL calculation 102 out of 210.0 possible\n",
      "Duration of last calculation: 0.006609916687011719\n",
      "Performing FL calculation 103 out of 210.0 possible\n",
      "Duration of last calculation: 0.0038559436798095703\n",
      "Performing FL calculation 104 out of 210.0 possible\n",
      "Duration of last calculation: 0.003701925277709961\n",
      "Performing FL calculation 105 out of 210.0 possible\n",
      "Duration of last calculation: 0.0040760040283203125\n",
      "Performing FL calculation 106 out of 210.0 possible\n",
      "Duration of last calculation: 0.0076220035552978516\n",
      "Performing FL calculation 107 out of 210.0 possible\n",
      "Duration of last calculation: 0.004441022872924805\n",
      "Performing FL calculation 108 out of 210.0 possible\n",
      "Duration of last calculation: 0.004192829132080078\n",
      "Performing FL calculation 109 out of 210.0 possible\n",
      "Duration of last calculation: 0.0040700435638427734\n",
      "Performing FL calculation 110 out of 210.0 possible\n",
      "Duration of last calculation: 0.005298137664794922\n",
      "Performing FL calculation 111 out of 210.0 possible\n",
      "Duration of last calculation: 0.006350994110107422\n",
      "Performing FL calculation 112 out of 210.0 possible\n",
      "Duration of last calculation: 0.003545999526977539\n",
      "Performing FL calculation 113 out of 210.0 possible\n",
      "Duration of last calculation: 0.0031490325927734375\n",
      "Performing FL calculation 114 out of 210.0 possible\n",
      "Duration of last calculation: 0.003065824508666992\n",
      "Performing FL calculation 115 out of 210.0 possible\n",
      "Duration of last calculation: 0.003392934799194336\n",
      "Performing FL calculation 116 out of 210.0 possible\n",
      "Duration of last calculation: 0.0037941932678222656\n",
      "Performing FL calculation 117 out of 210.0 possible\n",
      "Duration of last calculation: 0.003492116928100586\n",
      "Performing FL calculation 118 out of 210.0 possible\n",
      "Duration of last calculation: 0.002989053726196289\n",
      "Performing FL calculation 119 out of 210.0 possible\n",
      "Duration of last calculation: 0.003077983856201172\n",
      "Performing FL calculation 120 out of 210.0 possible\n",
      "Duration of last calculation: 0.0037288665771484375\n",
      "Performing FL calculation 121 out of 210.0 possible\n",
      "Duration of last calculation: 0.0037381649017333984\n",
      "Performing FL calculation 122 out of 210.0 possible\n",
      "Duration of last calculation: 0.003376007080078125\n",
      "Performing FL calculation 123 out of 210.0 possible\n",
      "Duration of last calculation: 0.003262042999267578\n",
      "Performing FL calculation 124 out of 210.0 possible\n",
      "Duration of last calculation: 0.0035190582275390625\n",
      "Performing FL calculation 125 out of 210.0 possible\n",
      "Duration of last calculation: 0.004284858703613281\n",
      "Performing FL calculation 126 out of 210.0 possible\n",
      "Duration of last calculation: 0.003194093704223633\n",
      "Performing FL calculation 127 out of 210.0 possible\n",
      "Duration of last calculation: 0.0030510425567626953\n",
      "Performing FL calculation 128 out of 210.0 possible\n",
      "Duration of last calculation: 0.003084897994995117\n",
      "Performing FL calculation 129 out of 210.0 possible\n",
      "Duration of last calculation: 0.003448963165283203\n",
      "Performing FL calculation 130 out of 210.0 possible\n",
      "Duration of last calculation: 0.003498077392578125\n",
      "Performing FL calculation 131 out of 210.0 possible\n",
      "Duration of last calculation: 0.0030078887939453125\n",
      "Performing FL calculation 132 out of 210.0 possible\n",
      "Duration of last calculation: 0.002980947494506836\n",
      "Performing FL calculation 133 out of 210.0 possible\n",
      "Duration of last calculation: 0.0035979747772216797\n",
      "Performing FL calculation 134 out of 210.0 possible\n",
      "Duration of last calculation: 0.005872964859008789\n",
      "Performing FL calculation 135 out of 210.0 possible\n",
      "Duration of last calculation: 0.003687143325805664\n",
      "Performing FL calculation 136 out of 210.0 possible\n",
      "Duration of last calculation: 0.004960060119628906\n",
      "Performing FL calculation 137 out of 210.0 possible\n",
      "Duration of last calculation: 0.0035419464111328125\n",
      "Performing FL calculation 138 out of 210.0 possible\n",
      "Duration of last calculation: 0.003818988800048828\n",
      "Performing FL calculation 139 out of 210.0 possible\n",
      "Duration of last calculation: 0.0038459300994873047\n",
      "Performing FL calculation 140 out of 210.0 possible\n",
      "Duration of last calculation: 0.003473997116088867\n",
      "Performing FL calculation 141 out of 210.0 possible\n",
      "Duration of last calculation: 0.003487110137939453\n",
      "Performing FL calculation 142 out of 210.0 possible\n",
      "Duration of last calculation: 0.0038061141967773438\n",
      "Performing FL calculation 143 out of 210.0 possible\n",
      "Duration of last calculation: 0.0036079883575439453\n",
      "Performing FL calculation 144 out of 210.0 possible\n",
      "Duration of last calculation: 0.003137826919555664\n",
      "Performing FL calculation 145 out of 210.0 possible\n",
      "Duration of last calculation: 0.003576993942260742\n",
      "Performing FL calculation 146 out of 210.0 possible\n",
      "Duration of last calculation: 0.0035941600799560547\n",
      "Performing FL calculation 147 out of 210.0 possible\n",
      "Duration of last calculation: 0.0041577816009521484\n",
      "Performing FL calculation 148 out of 210.0 possible\n",
      "Duration of last calculation: 0.003370046615600586\n",
      "Performing FL calculation 149 out of 210.0 possible\n",
      "Duration of last calculation: 0.0035071372985839844\n",
      "Performing FL calculation 150 out of 210.0 possible\n",
      "Duration of last calculation: 0.003404855728149414\n",
      "Performing FL calculation 151 out of 210.0 possible\n",
      "Duration of last calculation: 0.003389120101928711\n",
      "Performing FL calculation 152 out of 210.0 possible\n",
      "Duration of last calculation: 0.003880023956298828\n",
      "Performing FL calculation 153 out of 210.0 possible\n",
      "Duration of last calculation: 0.004055023193359375\n",
      "Performing FL calculation 154 out of 210.0 possible\n",
      "Duration of last calculation: 0.0035500526428222656\n",
      "Performing FL calculation 155 out of 210.0 possible\n",
      "Duration of last calculation: 0.0035729408264160156\n",
      "Performing FL calculation 156 out of 210.0 possible\n",
      "Duration of last calculation: 0.0037059783935546875\n",
      "Performing FL calculation 157 out of 210.0 possible\n",
      "Duration of last calculation: 0.0044460296630859375\n",
      "Performing FL calculation 158 out of 210.0 possible\n",
      "Duration of last calculation: 0.0034332275390625\n",
      "Performing FL calculation 159 out of 210.0 possible\n",
      "Duration of last calculation: 0.003952980041503906\n",
      "Performing FL calculation 160 out of 210.0 possible\n",
      "Duration of last calculation: 0.0034401416778564453\n",
      "Performing FL calculation 161 out of 210.0 possible\n",
      "Duration of last calculation: 0.0044519901275634766\n",
      "Performing FL calculation 162 out of 210.0 possible\n",
      "Duration of last calculation: 0.0035390853881835938\n",
      "Performing FL calculation 163 out of 210.0 possible\n",
      "Duration of last calculation: 0.003359079360961914\n",
      "Performing FL calculation 164 out of 210.0 possible\n",
      "Duration of last calculation: 0.0035910606384277344\n",
      "Performing FL calculation 165 out of 210.0 possible\n",
      "Duration of last calculation: 0.0036661624908447266\n",
      "Performing FL calculation 166 out of 210.0 possible\n",
      "Duration of last calculation: 0.005260944366455078\n",
      "Performing FL calculation 167 out of 210.0 possible\n",
      "Duration of last calculation: 0.0066411495208740234\n",
      "Performing FL calculation 168 out of 210.0 possible\n",
      "Duration of last calculation: 0.0036249160766601562\n",
      "Performing FL calculation 169 out of 210.0 possible\n",
      "Duration of last calculation: 0.0034818649291992188\n",
      "Performing FL calculation 170 out of 210.0 possible\n",
      "Duration of last calculation: 0.0035669803619384766\n",
      "Performing FL calculation 171 out of 210.0 possible\n",
      "Duration of last calculation: 0.0035920143127441406\n",
      "Performing FL calculation 172 out of 210.0 possible\n",
      "Duration of last calculation: 0.003673076629638672\n",
      "Performing FL calculation 173 out of 210.0 possible\n",
      "Duration of last calculation: 0.003506898880004883\n",
      "Performing FL calculation 174 out of 210.0 possible\n",
      "Duration of last calculation: 0.0032629966735839844\n",
      "Performing FL calculation 175 out of 210.0 possible\n",
      "Duration of last calculation: 0.007066011428833008\n",
      "Performing FL calculation 176 out of 210.0 possible\n",
      "Duration of last calculation: 0.0055119991302490234\n",
      "Performing FL calculation 177 out of 210.0 possible\n",
      "Duration of last calculation: 0.004027843475341797\n",
      "Performing FL calculation 178 out of 210.0 possible\n",
      "Duration of last calculation: 0.003554821014404297\n",
      "Performing FL calculation 179 out of 210.0 possible\n",
      "Duration of last calculation: 0.004324913024902344\n",
      "Performing FL calculation 180 out of 210.0 possible\n",
      "Duration of last calculation: 0.0038619041442871094\n",
      "Performing FL calculation 181 out of 210.0 possible\n",
      "Duration of last calculation: 0.004890918731689453\n",
      "Performing FL calculation 182 out of 210.0 possible\n",
      "Duration of last calculation: 0.0038938522338867188\n",
      "Performing FL calculation 183 out of 210.0 possible\n",
      "Duration of last calculation: 0.004525184631347656\n",
      "Performing FL calculation 184 out of 210.0 possible\n",
      "Duration of last calculation: 0.003668069839477539\n",
      "Performing FL calculation 185 out of 210.0 possible\n",
      "Duration of last calculation: 0.003665924072265625\n",
      "Performing FL calculation 186 out of 210.0 possible\n",
      "Duration of last calculation: 0.0038950443267822266\n",
      "Performing FL calculation 187 out of 210.0 possible\n",
      "Duration of last calculation: 0.0033969879150390625\n",
      "Performing FL calculation 188 out of 210.0 possible\n",
      "Duration of last calculation: 0.003451108932495117\n",
      "Performing FL calculation 189 out of 210.0 possible\n",
      "Duration of last calculation: 0.003442049026489258\n",
      "Performing FL calculation 190 out of 210.0 possible\n",
      "Duration of last calculation: 0.0043718814849853516\n",
      "Performing FL calculation 191 out of 210.0 possible\n",
      "Duration of last calculation: 0.003407001495361328\n",
      "Performing FL calculation 192 out of 210.0 possible\n",
      "Duration of last calculation: 0.0034329891204833984\n",
      "Performing FL calculation 193 out of 210.0 possible\n",
      "Duration of last calculation: 0.0035228729248046875\n",
      "Performing FL calculation 194 out of 210.0 possible\n",
      "Duration of last calculation: 0.0036377906799316406\n",
      "Performing FL calculation 195 out of 210.0 possible\n",
      "Duration of last calculation: 0.0031549930572509766\n",
      "Performing FL calculation 196 out of 210.0 possible\n",
      "Duration of last calculation: 0.0042171478271484375\n",
      "Performing FL calculation 197 out of 210.0 possible\n",
      "Duration of last calculation: 0.008173942565917969\n",
      "Performing FL calculation 198 out of 210.0 possible\n",
      "Duration of last calculation: 0.007048845291137695\n",
      "Performing FL calculation 199 out of 210.0 possible\n",
      "Duration of last calculation: 0.005689859390258789\n",
      "Performing FL calculation 200 out of 210.0 possible\n",
      "Duration of last calculation: 0.0038530826568603516\n",
      "Performing FL calculation 201 out of 210.0 possible\n",
      "Duration of last calculation: 0.003777027130126953\n",
      "Performing FL calculation 202 out of 210.0 possible\n",
      "Duration of last calculation: 0.00409698486328125\n",
      "Performing FL calculation 203 out of 210.0 possible\n",
      "Duration of last calculation: 0.003587961196899414\n",
      "Performing FL calculation 204 out of 210.0 possible\n",
      "Duration of last calculation: 0.004012107849121094\n",
      "Performing FL calculation 205 out of 210.0 possible\n",
      "Duration of last calculation: 0.005391120910644531\n",
      "Performing FL calculation 206 out of 210.0 possible\n",
      "Duration of last calculation: 0.0035619735717773438\n",
      "Performing FL calculation 207 out of 210.0 possible\n",
      "Duration of last calculation: 0.0035021305084228516\n",
      "Performing FL calculation 208 out of 210.0 possible\n",
      "Duration of last calculation: 0.004415035247802734\n",
      "Performing FL calculation 209 out of 210.0 possible\n",
      "Duration of last calculation: 0.003477811813354492\n",
      "Performing FL calculation 210 out of 210.0 possible\n",
      "Duration of last calculation: 0.004590034484863281\n"
     ]
    }
   ],
   "source": [
    "all_comparisons = all_pairwise_fls(corpus_context, relative_fl = False,\n",
    "                    algorithm = 'minpair',\n",
    "                    relative_count_to_relevant_sounds = False, relative_count_to_whole_corpus = True, distinguish_homophones = False,\n",
    "                    environment_filter = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0      1\n",
      "0    (t, z)  0.075\n",
      "1    (r, t)  0.075\n",
      "2    (r, z)  0.075\n",
      "3    (l, s)  0.050\n",
      "4    (l, n)  0.050\n",
      "5    (a, e)  0.050\n",
      "6    (r, s)  0.050\n",
      "7    (x, z)  0.025\n",
      "8    (p, z)  0.025\n",
      "9    (a, o)  0.025\n",
      "10   (s, t)  0.025\n",
      "11   (l, r)  0.025\n",
      "12   (l, z)  0.025\n",
      "13   (d, z)  0.025\n",
      "14   (e, i)  0.025\n",
      "15   (n, s)  0.025\n",
      "16   (f, i)  0.000\n",
      "17   (b, y)  0.000\n",
      "18   (j, p)  0.000\n",
      "19   (j, m)  0.000\n",
      "20   (d, e)  0.000\n",
      "21   (k, y)  0.000\n",
      "22   (l, p)  0.000\n",
      "23   (x, y)  0.000\n",
      "24   (d, o)  0.000\n",
      "25   (d, y)  0.000\n",
      "26   (i, y)  0.000\n",
      "27   (f, s)  0.000\n",
      "28   (e, w)  0.000\n",
      "29   (j, w)  0.000\n",
      "30   (e, y)  0.000\n",
      "31   (i, t)  0.000\n",
      "32   (l, y)  0.000\n",
      "33   (j, s)  0.000\n",
      "34   (p, t)  0.000\n",
      "35   (k, t)  0.000\n",
      "36   (e, x)  0.000\n",
      "37   (f, w)  0.000\n",
      "38   (d, j)  0.000\n",
      "39   (d, u)  0.000\n",
      "40   (f, p)  0.000\n",
      "41   (a, u)  0.000\n",
      "42   (a, j)  0.000\n",
      "43   (o, z)  0.000\n",
      "44   (j, o)  0.000\n",
      "45   (o, t)  0.000\n",
      "46   (a, p)  0.000\n",
      "47   (k, w)  0.000\n",
      "48   (r, u)  0.000\n",
      "49   (e, l)  0.000\n",
      "50   (j, l)  0.000\n",
      "51   (a, b)  0.000\n",
      "52   (s, u)  0.000\n",
      "53   (k, n)  0.000\n",
      "54   (m, y)  0.000\n",
      "55   (f, o)  0.000\n",
      "56   (e, p)  0.000\n",
      "57   (a, y)  0.000\n",
      "58   (l, w)  0.000\n",
      "59   (a, n)  0.000\n",
      "60   (y, z)  0.000\n",
      "61   (n, r)  0.000\n",
      "62   (m, n)  0.000\n",
      "63   (a, s)  0.000\n",
      "64   (o, x)  0.000\n",
      "65   (a, z)  0.000\n",
      "66   (t, u)  0.000\n",
      "67   (b, n)  0.000\n",
      "68   (f, x)  0.000\n",
      "69   (u, z)  0.000\n",
      "70   (l, u)  0.000\n",
      "71   (d, m)  0.000\n",
      "72   (a, d)  0.000\n",
      "73   (l, o)  0.000\n",
      "74   (s, z)  0.000\n",
      "75   (d, k)  0.000\n",
      "76   (a, t)  0.000\n",
      "77   (l, x)  0.000\n",
      "78   (s, w)  0.000\n",
      "79   (p, s)  0.000\n",
      "80   (r, x)  0.000\n",
      "81   (m, w)  0.000\n",
      "82   (d, r)  0.000\n",
      "83   (e, k)  0.000\n",
      "84   (d, w)  0.000\n",
      "85   (j, t)  0.000\n",
      "86   (e, m)  0.000\n",
      "87   (a, w)  0.000\n",
      "88   (b, e)  0.000\n",
      "89   (l, m)  0.000\n",
      "90   (e, n)  0.000\n",
      "91   (i, w)  0.000\n",
      "92   (w, x)  0.000\n",
      "93   (i, n)  0.000\n",
      "94   (w, y)  0.000\n",
      "95   (f, m)  0.000\n",
      "96   (n, p)  0.000\n",
      "97   (f, k)  0.000\n",
      "98   (j, u)  0.000\n",
      "99   (o, p)  0.000\n",
      "100  (s, x)  0.000\n",
      "101  (f, n)  0.000\n",
      "102  (a, i)  0.000\n",
      "103  (j, k)  0.000\n",
      "104  (m, r)  0.000\n",
      "105  (i, k)  0.000\n",
      "106  (r, y)  0.000\n",
      "107  (n, x)  0.000\n",
      "108  (j, y)  0.000\n",
      "109  (t, x)  0.000\n",
      "110  (a, f)  0.000\n",
      "111  (i, u)  0.000\n",
      "112  (b, w)  0.000\n",
      "113  (e, t)  0.000\n",
      "114  (d, l)  0.000\n",
      "115  (d, f)  0.000\n",
      "116  (n, z)  0.000\n",
      "117  (j, x)  0.000\n",
      "118  (f, z)  0.000\n",
      "119  (o, y)  0.000\n",
      "120  (t, y)  0.000\n",
      "121  (d, x)  0.000\n",
      "122  (n, y)  0.000\n",
      "123  (p, x)  0.000\n",
      "124  (b, k)  0.000\n",
      "125  (u, x)  0.000\n",
      "126  (o, w)  0.000\n",
      "127  (b, u)  0.000\n",
      "128  (j, z)  0.000\n",
      "129  (b, z)  0.000\n",
      "130  (n, w)  0.000\n",
      "131  (f, r)  0.000\n",
      "132  (i, s)  0.000\n",
      "133  (b, s)  0.000\n",
      "134  (i, l)  0.000\n",
      "135  (o, u)  0.000\n",
      "136  (m, u)  0.000\n",
      "137  (b, f)  0.000\n",
      "138  (e, f)  0.000\n",
      "139  (e, o)  0.000\n",
      "140  (j, r)  0.000\n",
      "141  (k, l)  0.000\n",
      "142  (m, p)  0.000\n",
      "143  (n, t)  0.000\n",
      "144  (d, t)  0.000\n",
      "145  (r, w)  0.000\n",
      "146  (w, z)  0.000\n",
      "147  (a, m)  0.000\n",
      "148  (f, l)  0.000\n",
      "149  (k, u)  0.000\n",
      "150  (k, m)  0.000\n",
      "151  (k, r)  0.000\n",
      "152  (i, z)  0.000\n",
      "153  (b, p)  0.000\n",
      "154  (t, w)  0.000\n",
      "155  (a, l)  0.000\n",
      "156  (f, y)  0.000\n",
      "157  (i, r)  0.000\n",
      "158  (m, z)  0.000\n",
      "159  (m, o)  0.000\n",
      "160  (d, s)  0.000\n",
      "161  (b, l)  0.000\n",
      "162  (l, t)  0.000\n",
      "163  (i, o)  0.000\n",
      "164  (d, n)  0.000\n",
      "165  (d, i)  0.000\n",
      "166  (m, s)  0.000\n",
      "167  (u, w)  0.000\n",
      "168  (m, t)  0.000\n",
      "169  (p, w)  0.000\n",
      "170  (k, o)  0.000\n",
      "171  (b, m)  0.000\n",
      "172  (k, x)  0.000\n",
      "173  (e, r)  0.000\n",
      "174  (e, z)  0.000\n",
      "175  (a, x)  0.000\n",
      "176  (u, y)  0.000\n",
      "177  (i, p)  0.000\n",
      "178  (i, x)  0.000\n",
      "179  (m, x)  0.000\n",
      "180  (d, p)  0.000\n",
      "181  (i, j)  0.000\n",
      "182  (b, r)  0.000\n",
      "183  (n, u)  0.000\n",
      "184  (k, p)  0.000\n",
      "185  (b, d)  0.000\n",
      "186  (a, r)  0.000\n",
      "187  (f, t)  0.000\n",
      "188  (b, o)  0.000\n",
      "189  (b, x)  0.000\n",
      "190  (p, y)  0.000\n",
      "191  (s, y)  0.000\n",
      "192  (o, s)  0.000\n",
      "193  (f, u)  0.000\n",
      "194  (a, k)  0.000\n",
      "195  (f, j)  0.000\n",
      "196  (p, u)  0.000\n",
      "197  (j, n)  0.000\n",
      "198  (b, i)  0.000\n",
      "199  (e, s)  0.000\n",
      "200  (i, m)  0.000\n",
      "201  (o, r)  0.000\n",
      "202  (k, z)  0.000\n",
      "203  (b, t)  0.000\n",
      "204  (e, u)  0.000\n",
      "205  (p, r)  0.000\n",
      "206  (n, o)  0.000\n",
      "207  (e, j)  0.000\n",
      "208  (k, s)  0.000\n",
      "209  (b, j)  0.000\n"
     ]
    }
   ],
   "source": [
    "pandas.options.display.max_rows = 215\n",
    "all_pairs_df = pandas.DataFrame(all_comparisons)\n",
    "print(all_pairs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_raw_fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'a']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ('s', 'a')\n",
    "list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 segment pairs containing [s].\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(segment_pairs_with_s), \"segment pairs containing [s].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(raw_s_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
